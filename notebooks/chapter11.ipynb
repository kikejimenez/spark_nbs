{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Learning Spark Second Edition](https://github.com/databricks/LearningSparkV2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " use _docker-compose.yml_ file located at he root directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Chapter 11](https://learning.oreilly.com/library/view/learning-spark-2nd/9781492050032/ch11.html)\n",
    "> MLflow: Track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Spark Session\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a SparkSession\n",
    "PARENT_DIR = os.popen('dirname $PWD').read().strip()\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Chapter 11. ML Flows\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# filepath\n",
    "filePath = 'databricks-datasets/learning-spark-v2/sf-airbnb/'+ \\\n",
    "          'sf-airbnb-clean.parquet/'\n",
    "filePath = os.path.join(PARENT_DIR,filePath)\n",
    "\n",
    "#load and create log price\n",
    "from pyspark.sql.functions import log\n",
    "airbnbDF = spark.read.parquet(filePath).withColumn('log_price', log('price'))\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "#train-test split\n",
    "trainDF, testDF = airbnbDF.randomSplit([.8, .2], seed=42)\n",
    "\n",
    "# Combine output of StringIndexer defined above and numeric columns\n",
    "from pyspark.ml.feature import VectorAssembler,StringIndexer\n",
    "\n",
    "categoricalCols = [field for (field, dataType) in trainDF.dtypes \n",
    "                   if dataType == \"string\"]\n",
    "\n",
    "indexOutputCols = [x + \"Index\" for x in categoricalCols]\n",
    "\n",
    "numericCols = [field for (field, dataType) in trainDF.dtypes \n",
    "               if ((dataType == \"double\") & (field != \"log_price\"))]\n",
    "\n",
    "stringIndexer = StringIndexer(inputCols=categoricalCols, \n",
    "                              outputCols=indexOutputCols, \n",
    "                              handleInvalid=\"skip\")\n",
    "\n",
    "assemblerInputs = indexOutputCols + numericCols\n",
    "vecAssembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\n",
    "\n",
    "from pyspark.ml.tuning import CrossValidator\n",
    "#Random Forest\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "rf = RandomForestRegressor(labelCol=\"log_price\", maxBins=40, seed=42)\n",
    "\n",
    "\n",
    "# pipeline\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "pipeline = Pipeline(stages = [stringIndexer, vecAssembler, rf])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Python \n",
    "import mlflow\n",
    "import mlflow.spark\n",
    "import pandas as pd\n",
    "\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "#from pyspark.ml.tuning import CrossValidator\n",
    "\n",
    "with mlflow.start_run(run_name=\"random-forest\") as run:\n",
    "    \n",
    "    # Log params: num_trees and max_depth\n",
    "    mlflow.log_param(\"num_trees\", rf.getNumTrees())\n",
    "    mlflow.log_param(\"max_depth\", rf.getMaxDepth())\n",
    "\n",
    "    # Log model\n",
    "    pipelineModel = pipeline.fit(trainDF)\n",
    "    mlflow.spark.log_model(pipelineModel, \"model\")\n",
    "\n",
    "    # Log metrics: RMSE and R2\n",
    "    predDF = pipelineModel.transform(testDF)\n",
    "    regressionEvaluator = RegressionEvaluator(predictionCol=\"prediction\", \n",
    "                                            labelCol=\"price\")\n",
    "    rmse = regressionEvaluator.setMetricName(\"rmse\").evaluate(predDF)\n",
    "    r2 = regressionEvaluator.setMetricName(\"r2\").evaluate(predDF)\n",
    "    mlflow.log_metrics({\"rmse\": rmse, \"r2\": r2})\n",
    "\n",
    "    # Log artifact: feature importance scores\n",
    "    rfModel = pipelineModel.stages[-1]\n",
    "    pandasDF = (pd.DataFrame(list(zip(vecAssembler.getInputCols(), \n",
    "                                    rfModel.featureImportances)), \n",
    "                           columns=[\"feature\", \"importance\"])\n",
    "              .sort_values(by=\"importance\", ascending=False))\n",
    "\n",
    "    # First write to local filesystem, then tell MLflow where to find that file\n",
    "    pandasDF.to_csv(\"feature-importance.csv\", index=False)\n",
    "    mlflow.log_artifact(\"feature-importance.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!pip install mlflow > /dev/null\n",
    "#!mlflow server --host 0.0.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-07-28 21:14:17 +0000] [923] [INFO] Starting gunicorn 20.0.4\n",
      "[2020-07-28 21:14:17 +0000] [923] [ERROR] Connection in use: ('127.0.0.1', 5000)\n",
      "[2020-07-28 21:14:17 +0000] [923] [ERROR] Retrying in 1 second.\n",
      "[2020-07-28 21:14:18 +0000] [923] [ERROR] Connection in use: ('127.0.0.1', 5000)\n",
      "[2020-07-28 21:14:18 +0000] [923] [ERROR] Retrying in 1 second.\n",
      "[2020-07-28 21:14:19 +0000] [923] [ERROR] Connection in use: ('127.0.0.1', 5000)\n",
      "[2020-07-28 21:14:19 +0000] [923] [ERROR] Retrying in 1 second.\n",
      "[2020-07-28 21:14:20 +0000] [923] [ERROR] Connection in use: ('127.0.0.1', 5000)\n",
      "[2020-07-28 21:14:20 +0000] [923] [ERROR] Retrying in 1 second.\n",
      "[2020-07-28 21:14:21 +0000] [923] [ERROR] Connection in use: ('127.0.0.1', 5000)\n",
      "[2020-07-28 21:14:21 +0000] [923] [ERROR] Retrying in 1 second.\n",
      "[2020-07-28 21:14:22 +0000] [923] [ERROR] Can't connect to ('127.0.0.1', 5000)\n",
      "Running the mlflow server failed. Please see the logs above for details.\n"
     ]
    }
   ],
   "source": [
    "#port http://127.0.0.1:5000/\n",
    "!mlflow ui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "search_runs() got an unexpected keyword argument 'order_by'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-1bdc00f16e68>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mclient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMlflowClient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m runs = client.search_runs(run.info.experiment_id, \n\u001b[0m\u001b[1;32m      6\u001b[0m                           \u001b[0morder_by\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"attributes.start_time desc\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m                           max_results=1)\n",
      "\u001b[0;31mTypeError\u001b[0m: search_runs() got an unexpected keyword argument 'order_by'"
     ]
    }
   ],
   "source": [
    "# In Python\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "client = MlflowClient()\n",
    "runs = client.search_runs(run.info.experiment_id, \n",
    "                          order_by=[\"attributes.start_time desc\"], \n",
    "                          max_results=1)\n",
    "\n",
    "run_id = runs[0].info.run_id\n",
    "runs[0].data.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
