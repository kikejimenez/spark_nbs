{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Learning Spark Second Edition](https://github.com/databricks/LearningSparkV2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_almond_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                   // Or use any other 2.x version here\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql._\u001b[39m"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`org.apache.spark::spark-sql:3.0.0` // Or use any other 2.x version here\n",
    "import org.apache.spark.sql._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.log4j.{Level, Logger}\n",
       "\u001b[39m"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.log4j.{Level, Logger}\n",
    "Logger.getLogger(\"org\").setLevel(Level.OFF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Chapter 8](https://learning.oreilly.com/library/view/Learning+Spark,+2nd+Edition/9781492050032/ch08.html#executor_memory_layout)\n",
    "> Optimizing and Tuning Spark for Efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.SparkSession\n",
       "\n",
       "\u001b[39m\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36mprintConfigs\u001b[39m\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36mmain\u001b[39m"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// In Scala\n",
    "import org.apache.spark.sql.SparkSession\n",
    "\n",
    "def printConfigs(session: SparkSession) = {\n",
    "   // Get conf\n",
    "   val mconf = session.conf.getAll\n",
    "   // Print them\n",
    "   for (k <- mconf.keySet) { println(s\"${k} -> ${mconf(k)}\\n\") }\n",
    "}\n",
    "\n",
    "def main(args: Array[String]) {\n",
    " // Create a session\n",
    " val spark = SparkSession.builder\n",
    "   .config(\"spark.sql.shuffle.partitions\", 5)\n",
    "   .config(\"spark.executor.memory\", \"2g\")\n",
    "   .master(\"local[*]\")\n",
    "   .appName(\"SparkConfig\")\n",
    "   .getOrCreate()\n",
    "\n",
    " printConfigs(spark)\n",
    " spark.conf.set(\"spark.sql.shuffle.partitions\",\n",
    "   spark.sparkContext.defaultParallelism)\n",
    " println(\" ****** Setting Shuffle Partitions to Default Parallelism\")\n",
    " printConfigs(spark)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark.driver.host -> 623c4a2181fc\n",
      "\n",
      "spark.driver.port -> 36607\n",
      "\n",
      "spark.app.name -> SparkConfig\n",
      "\n",
      "spark.executor.id -> driver\n",
      "\n",
      "spark.master -> local[*]\n",
      "\n",
      "spark.executor.memory -> 2g\n",
      "\n",
      "spark.app.id -> local-1595367206220\n",
      "\n",
      "spark.sql.shuffle.partitions -> 5\n",
      "\n",
      " ****** Setting Shuffle Partitions to Default Parallelism\n",
      "spark.driver.host -> 623c4a2181fc\n",
      "\n",
      "spark.driver.port -> 36607\n",
      "\n",
      "spark.app.name -> SparkConfig\n",
      "\n",
      "spark.executor.id -> driver\n",
      "\n",
      "spark.master -> local[*]\n",
      "\n",
      "spark.executor.memory -> 2g\n",
      "\n",
      "spark.app.id -> local-1595367206220\n",
      "\n",
      "spark.sql.shuffle.partitions -> 8\n",
      "\n"
     ]
    }
   ],
   "source": [
    "main(Array[String]())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Chapter 8](https://learning.oreilly.com/library/view/Learning+Spark,+2nd+Edition/9781492050032/ch08.html#executor_memory_layout)\n",
    "> A Family of Spark Joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.SparkSession\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.types._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.functions._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.SaveMode\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mscala.util.Random\n",
       "\n",
       "  // curried function to benchmark any code or function\n",
       "  \u001b[39m\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36mbenchmark\u001b[39m\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36mmain\u001b[39m"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Shuffle Sort Merge Join\n",
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.SaveMode\n",
    "import scala.util.Random\n",
    "\n",
    "  // curried function to benchmark any code or function\n",
    "  def benchmark(name: String)(f: => Unit) {\n",
    "    val startTime = System.nanoTime\n",
    "    f\n",
    "    val endTime = System.nanoTime\n",
    "    println(s\"Time taken in $name: \" + (endTime - startTime).toDouble / 1000000000 + \" seconds\")\n",
    "  }\n",
    "\n",
    "  // main class setting the configs\n",
    "  def main (args: Array[String] ) {\n",
    "\n",
    "    val spark = SparkSession.builder\n",
    "        .appName(\"SortMergeJoin\")\n",
    "        .config(\"spark.sql.codegen.wholeStage\", true)\n",
    "        .config(\"spark.sql.join.preferSortMergeJoin\", true)\n",
    "        .config(\"spark.sql.autoBroadcastJoinThreshold\", -1)\n",
    "        .config(\"spark.sql.defaultSizeInBytes\", 100000)\n",
    "        .config(\"spark.sql.shuffle.partitions\", 16)\n",
    "        .getOrCreate ()\n",
    "\n",
    "    import spark.implicits._\n",
    "\n",
    "    var states = scala.collection.mutable.Map[Int, String]()\n",
    "    var items = scala.collection.mutable.Map[Int, String]()\n",
    "    val rnd = new scala.util.Random(42)\n",
    "\n",
    "    // initialize states and items purchased\n",
    "    states += (0 -> \"AZ\", 1 -> \"CO\", 2-> \"CA\", 3-> \"TX\", 4 -> \"NY\", 5-> \"MI\")\n",
    "    items += (0 -> \"SKU-0\", 1 -> \"SKU-1\", 2-> \"SKU-2\", 3-> \"SKU-3\", 4 -> \"SKU-4\", 5-> \"SKU-5\")\n",
    "    // create dataframes\n",
    "    val usersDF = (0 to 100000).map(id => (id, s\"user_${id}\", s\"user_${id}@databricks.com\", \n",
    "                                           states(rnd.nextInt(5))))\n",
    "          .toDF(\"uid\", \"login\", \"email\", \"user_state\")\n",
    "    val ordersDF = (0 to 100000).map(r => (r, r, rnd.nextInt(100000), 10 * r* 0.2d,\n",
    "                                           states(rnd.nextInt(5)), items(rnd.nextInt(5))))\n",
    "          .toDF(\"transaction_id\", \"quantity\", \"users_id\", \"amount\", \"state\", \"items\")\n",
    "\n",
    "    usersDF.show(10)\n",
    "    ordersDF.show(10)\n",
    "\n",
    "    // do a Join\n",
    "    val usersOrdersDF = ordersDF.join(usersDF, $\"users_id\" === $\"uid\")\n",
    "    usersOrdersDF.show(10, false)\n",
    "    usersOrdersDF.cache()\n",
    "    usersOrdersDF.explain()\n",
    "    // usersOrdersDF.explain(\"formated\")\n",
    "    // uncoment to view the SparkUI otherwise the program terminates and shutdowsn the UI\n",
    "    // Thread.sleep(200000000)\n",
    "  }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+--------------------+----------+\n",
      "|uid| login|               email|user_state|\n",
      "+---+------+--------------------+----------+\n",
      "|  0|user_0|user_0@databricks...|        AZ|\n",
      "|  1|user_1|user_1@databricks...|        TX|\n",
      "|  2|user_2|user_2@databricks...|        TX|\n",
      "|  3|user_3|user_3@databricks...|        NY|\n",
      "|  4|user_4|user_4@databricks...|        AZ|\n",
      "|  5|user_5|user_5@databricks...|        AZ|\n",
      "|  6|user_6|user_6@databricks...|        AZ|\n",
      "|  7|user_7|user_7@databricks...|        TX|\n",
      "|  8|user_8|user_8@databricks...|        NY|\n",
      "|  9|user_9|user_9@databricks...|        TX|\n",
      "+---+------+--------------------+----------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+--------------+--------+--------+------+-----+-----+\n",
      "|transaction_id|quantity|users_id|amount|state|items|\n",
      "+--------------+--------+--------+------+-----+-----+\n",
      "|             0|       0|   78432|   0.0|   AZ|SKU-0|\n",
      "|             1|       1|   84041|   2.0|   AZ|SKU-3|\n",
      "|             2|       2|   34657|   4.0|   NY|SKU-1|\n",
      "|             3|       3|   67210|   6.0|   NY|SKU-2|\n",
      "|             4|       4|    6062|   8.0|   TX|SKU-4|\n",
      "|             5|       5|   50274|  10.0|   CA|SKU-2|\n",
      "|             6|       6|   19305|  12.0|   CA|SKU-3|\n",
      "|             7|       7|   81370|  14.0|   CO|SKU-3|\n",
      "|             8|       8|   30000|  16.0|   NY|SKU-0|\n",
      "|             9|       9|   73563|  18.0|   CO|SKU-3|\n",
      "+--------------+--------+--------+------+-----+-----+\n",
      "only showing top 10 rows\n",
      "\n",
      "+--------------+--------+--------+--------+-----+-----+---+--------+-----------------------+----------+\n",
      "|transaction_id|quantity|users_id|amount  |state|items|uid|login   |email                  |user_state|\n",
      "+--------------+--------+--------+--------+-----+-----+---+--------+-----------------------+----------+\n",
      "|5436          |5436    |18      |10872.0 |CA   |SKU-3|18 |user_18 |user_18@databricks.com |TX        |\n",
      "|47648         |47648   |38      |95296.0 |NY   |SKU-3|38 |user_38 |user_38@databricks.com |TX        |\n",
      "|77507         |77507   |38      |155014.0|NY   |SKU-2|38 |user_38 |user_38@databricks.com |TX        |\n",
      "|10811         |10811   |67      |21622.0 |NY   |SKU-0|67 |user_67 |user_67@databricks.com |NY        |\n",
      "|25891         |25891   |67      |51782.0 |AZ   |SKU-4|67 |user_67 |user_67@databricks.com |NY        |\n",
      "|2752          |2752    |70      |5504.0  |AZ   |SKU-4|70 |user_70 |user_70@databricks.com |AZ        |\n",
      "|56463         |56463   |93      |112926.0|NY   |SKU-1|93 |user_93 |user_93@databricks.com |AZ        |\n",
      "|30402         |30402   |161     |60804.0 |CA   |SKU-1|161|user_161|user_161@databricks.com|CO        |\n",
      "|61442         |61442   |161     |122884.0|AZ   |SKU-2|161|user_161|user_161@databricks.com|CO        |\n",
      "|25261         |25261   |186     |50522.0 |CA   |SKU-2|186|user_186|user_186@databricks.com|CO        |\n",
      "+--------------+--------+--------+--------+-----+-----+---+--------+-----------------------+----------+\n",
      "only showing top 10 rows\n",
      "\n",
      "== Physical Plan ==\n",
      "InMemoryTableScan [transaction_id#40, quantity#41, users_id#42, amount#43, state#44, items#45, uid#13, login#14, email#15, user_state#16]\n",
      "   +- InMemoryRelation [transaction_id#40, quantity#41, users_id#42, amount#43, state#44, items#45, uid#13, login#14, email#15, user_state#16], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "         +- *(3) SortMergeJoin [users_id#42], [uid#13], Inner\n",
      "            :- *(1) Sort [users_id#42 ASC NULLS FIRST], false, 0\n",
      "            :  +- Exchange hashpartitioning(users_id#42, 16), true, [id=#56]\n",
      "            :     +- LocalTableScan [transaction_id#40, quantity#41, users_id#42, amount#43, state#44, items#45]\n",
      "            +- *(2) Sort [uid#13 ASC NULLS FIRST], false, 0\n",
      "               +- Exchange hashpartitioning(uid#13, 16), true, [id=#57]\n",
      "                  +- LocalTableScan [uid#13, login#14, email#15, user_state#16]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "main(Array[String]())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.SparkSession\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.types._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.functions._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.storage.StorageLevel._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.SaveMode\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mscala.util.Random\n",
       "\n",
       "\n",
       "  // curried function to benchmark any code or function\n",
       "  \u001b[39m\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36mbenchmark\u001b[39m\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36mmain\u001b[39m"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//OPTIMIZING THE SHUFFLE SORT MERGE JOIN\n",
    "\n",
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.storage.StorageLevel._\n",
    "import org.apache.spark.sql.SaveMode\n",
    "import scala.util.Random\n",
    "\n",
    "\n",
    "  // curried function to benchmark any code or function\n",
    "  def benchmark(name: String)(f: => Unit) {\n",
    "    val startTime = System.nanoTime\n",
    "    f\n",
    "    val endTime = System.nanoTime\n",
    "    println(s\"Time taken in $name: \" + (endTime - startTime).toDouble / 1000000000 + \" seconds\")\n",
    "  }\n",
    "\n",
    "  // main class setting the configs\n",
    "  def main (args: Array[String] ) {\n",
    "\n",
    "    val spark = SparkSession.builder\n",
    "        .appName(\"SortMergeJoinBucketed\")\n",
    "        .config(\"spark.sql.codegen.wholeStage\", true)\n",
    "        .config(\"spark.sql.join.preferSortMergeJoin\", true)\n",
    "        .config(\"spark.sql.autoBroadcastJoinThreshold\", -1)\n",
    "        .config(\"spark.sql.defaultSizeInBytes\", 100000)\n",
    "        .config(\"spark.sql.shuffle.partitions\", 16)\n",
    "        .getOrCreate ()\n",
    "\n",
    "    import spark.implicits._\n",
    "\n",
    "    var states = scala.collection.mutable.Map[Int, String]()\n",
    "    var items = scala.collection.mutable.Map[Int, String]()\n",
    "    val rnd = new scala.util.Random(42)\n",
    "\n",
    "    // initialize states and items purchased\n",
    "    states += (0 -> \"AZ\", 1 -> \"CO\", 2-> \"CA\", 3-> \"TX\", 4 -> \"NY\", 5-> \"MI\")\n",
    "    items += (0 -> \"SKU-0\", 1 -> \"SKU-1\", 2-> \"SKU-2\", 3-> \"SKU-3\", 4 -> \"SKU-4\", 5-> \"SKU-5\")\n",
    "    // create dataframes\n",
    "    val usersDF = (0 to 100000).map(id => (id, s\"user_${id}\", s\"user_${id}@databricks.com\", states(rnd.nextInt(5))))\n",
    "          .toDF(\"uid\", \"login\", \"email\", \"user_state\")\n",
    "    val ordersDF = (0 to 100000).map(r => (r, r, rnd.nextInt(100000), 10 * r* 0.2d, states(rnd.nextInt(5)), items(rnd.nextInt(5))))\n",
    "          .toDF(\"transaction_id\", \"quantity\", \"users_id\", \"amount\", \"state\", \"items\")\n",
    "\n",
    "    // cache them on Disk only so we can see the difference in size in the storage UI\n",
    "    usersDF.persist(DISK_ONLY)\n",
    "    ordersDF.persist(DISK_ONLY)\n",
    "\n",
    "    // let's create five buckets, each DataFrame for their respective columns\n",
    "    // create bucket and table for uid\n",
    "    spark.sql(\"DROP TABLE IF EXISTS UsersTbl\")\n",
    "    usersDF.orderBy(asc(\"uid\"))\n",
    "      .write.format(\"parquet\")\n",
    "      .mode(SaveMode.Overwrite)\n",
    "      // eual to number of cores I have on my laptop\n",
    "      .bucketBy(8, \"uid\")\n",
    "      .saveAsTable(\"UsersTbl\")\n",
    "      // create bucket and table for users_id\n",
    "    spark.sql(\"DROP TABLE IF EXISTS OrdersTbl\")\n",
    "    ordersDF.orderBy(asc(\"users_id\"))\n",
    "      .write.format(\"parquet\")\n",
    "      .bucketBy(8, \"users_id\")\n",
    "      .mode(SaveMode.Overwrite)\n",
    "      .saveAsTable(\"OrdersTbl\")\n",
    "    // cache tables in memory so that we can see the difference in size in the storage UI\n",
    "    spark.sql(\"CACHE TABLE UsersTbl\")\n",
    "    spark.sql(\"CACHE TABLE OrdersTbl\")\n",
    "    spark.sql(\"SELECT * from OrdersTbl LIMIT 20\")\n",
    "    // read data back in\n",
    "    val usersBucketDF = spark.table(\"UsersTbl\")\n",
    "    val ordersBucketDF = spark.table(\"OrdersTbl\")\n",
    "    // Now do the join on the bucketed DataFrames\n",
    "    val joinUsersOrdersBucketDF = ordersBucketDF.join(usersBucketDF, $\"users_id\" === $\"uid\")\n",
    "    joinUsersOrdersBucketDF.show(false)\n",
    "    joinUsersOrdersBucketDF.explain()\n",
    "    //joinUsersOrdersBucketDF.explain(\"formatted\")\n",
    "\n",
    "    // uncomment to view the SparkUI otherwise the program terminates and shutdowsn the UI\n",
    "    // Thread.sleep(200000000)\n",
    "  }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------+--------+--------+-----+-----+---+--------+-----------------------+----------+\n",
      "|transaction_id|quantity|users_id|amount  |state|items|uid|login   |email                  |user_state|\n",
      "+--------------+--------+--------+--------+-----+-----+---+--------+-----------------------+----------+\n",
      "|85775         |85775   |13      |171550.0|AZ   |SKU-2|13 |user_13 |user_13@databricks.com |CA        |\n",
      "|79730         |79730   |14      |159460.0|AZ   |SKU-0|14 |user_14 |user_14@databricks.com |CO        |\n",
      "|5436          |5436    |18      |10872.0 |CA   |SKU-3|18 |user_18 |user_18@databricks.com |TX        |\n",
      "|47648         |47648   |38      |95296.0 |NY   |SKU-3|38 |user_38 |user_38@databricks.com |TX        |\n",
      "|77507         |77507   |38      |155014.0|NY   |SKU-2|38 |user_38 |user_38@databricks.com |TX        |\n",
      "|50588         |50588   |46      |101176.0|CA   |SKU-1|46 |user_46 |user_46@databricks.com |CA        |\n",
      "|10811         |10811   |67      |21622.0 |NY   |SKU-0|67 |user_67 |user_67@databricks.com |NY        |\n",
      "|25891         |25891   |67      |51782.0 |AZ   |SKU-4|67 |user_67 |user_67@databricks.com |NY        |\n",
      "|2752          |2752    |70      |5504.0  |AZ   |SKU-4|70 |user_70 |user_70@databricks.com |AZ        |\n",
      "|56463         |56463   |93      |112926.0|NY   |SKU-1|93 |user_93 |user_93@databricks.com |AZ        |\n",
      "|5863          |5863    |107     |11726.0 |TX   |SKU-3|107|user_107|user_107@databricks.com|CO        |\n",
      "|34081         |34081   |107     |68162.0 |NY   |SKU-4|107|user_107|user_107@databricks.com|CO        |\n",
      "|30429         |30429   |157     |60858.0 |NY   |SKU-1|157|user_157|user_157@databricks.com|CO        |\n",
      "|37804         |37804   |157     |75608.0 |CO   |SKU-4|157|user_157|user_157@databricks.com|CO        |\n",
      "|95654         |95654   |157     |191308.0|TX   |SKU-2|157|user_157|user_157@databricks.com|CO        |\n",
      "|30402         |30402   |161     |60804.0 |CA   |SKU-1|161|user_161|user_161@databricks.com|CO        |\n",
      "|61442         |61442   |161     |122884.0|AZ   |SKU-2|161|user_161|user_161@databricks.com|CO        |\n",
      "|25261         |25261   |186     |50522.0 |CA   |SKU-2|186|user_186|user_186@databricks.com|CO        |\n",
      "|37370         |37370   |186     |74740.0 |TX   |SKU-4|186|user_186|user_186@databricks.com|CO        |\n",
      "|1848          |1848    |202     |3696.0  |CA   |SKU-4|202|user_202|user_202@databricks.com|TX        |\n",
      "+--------------+--------+--------+--------+-----+-----+---+--------+-----------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "== Physical Plan ==\n",
      "*(3) SortMergeJoin [users_id#730], [uid#627], Inner\n",
      ":- *(1) Sort [users_id#730 ASC NULLS FIRST], false, 0\n",
      ":  +- *(1) Filter isnotnull(users_id#730)\n",
      ":     +- Scan In-memory table `default`.`OrdersTbl` [transaction_id#728, quantity#729, users_id#730, amount#731, state#732, items#733], [isnotnull(users_id#730)]\n",
      ":           +- InMemoryRelation [transaction_id#728, quantity#729, users_id#730, amount#731, state#732, items#733], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      ":                 +- *(1) ColumnarToRow\n",
      ":                    +- FileScan parquet default.orderstbl[transaction_id#728,quantity#729,users_id#730,amount#731,state#732,items#733] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex[file:/wd/LearningSparkV2/notebooks/spark-warehouse/orderstbl], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<transaction_id:int,quantity:int,users_id:int,amount:double,state:string,items:string>, SelectedBucketsCount: 8 out of 8\n",
      "+- *(2) Sort [uid#627 ASC NULLS FIRST], false, 0\n",
      "   +- *(2) Filter isnotnull(uid#627)\n",
      "      +- Scan In-memory table `default`.`UsersTbl` [uid#627, login#628, email#629, user_state#630], [isnotnull(uid#627)]\n",
      "            +- InMemoryRelation [uid#627, login#628, email#629, user_state#630], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "                  +- *(1) ColumnarToRow\n",
      "                     +- FileScan parquet default.userstbl[uid#627,login#628,email#629,user_state#630] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex[file:/wd/LearningSparkV2/notebooks/spark-warehouse/userstbl], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<uid:int,login:string,email:string,user_state:string>, SelectedBucketsCount: 8 out of 8\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "main(Array[String]())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala 2.12",
   "language": "scala",
   "name": "scala212"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".sc",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
