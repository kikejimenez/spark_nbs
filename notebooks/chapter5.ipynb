{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Learning Spark Second Edition](https://github.com/databricks/LearningSparkV2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_all-spark-notebook_ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Chapter Five](https://learning.oreilly.com/library/view/learning-spark-2nd/9781492050032/ch05.html)\n",
    "> User Defined Functions (UDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import LongType\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "# Create cubed function\n",
    "def cubed(s):\n",
    "    return s * s * s\n",
    "\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = (SparkSession.builder.appName(\"SparkSQLExampleApp\").getOrCreate())\n",
    "\n",
    "# Register UDF\n",
    "spark.udf.register(\"cubed\", cubed, LongType())\n",
    "\n",
    "# Generate temporary view\n",
    "spark.range(1, 9).createOrReplaceTempView(\"udf_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+\n",
      "| id|id_cubed|\n",
      "+---+--------+\n",
      "|  1|       1|\n",
      "|  2|       8|\n",
      "|  3|      27|\n",
      "|  4|      64|\n",
      "|  5|     125|\n",
      "|  6|     216|\n",
      "|  7|     343|\n",
      "|  8|     512|\n",
      "+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Query the cubed UDF\n",
    "spark.sql(\"SELECT id, cubed(id) AS id_cubed FROM udf_test\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Chapter Five](https://learning.oreilly.com/library/view/learning-spark-2nd/9781492050032/ch05.html)\n",
    "> Pandas UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------+\n",
      "| id|((id * id) * id)|\n",
      "+---+----------------+\n",
      "|  1|               1|\n",
      "|  2|               8|\n",
      "|  3|              27|\n",
      "+---+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Import various pyspark SQL functions including pandas_udf\n",
    "from pyspark.sql.types import LongType\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, pandas_udf\n",
    "# Create a SparkSession\n",
    "spark = (SparkSession.builder.appName(\"SparkSQLExampleApp\").getOrCreate())\n",
    "\n",
    "\n",
    "# Declare the cubed function\n",
    "def cubed(a: pd.Series) -> pd.Series:\n",
    "    return a * a * a\n",
    "\n",
    "\n",
    "# Create the pandas UDF for the cubed function\n",
    "cubed_udf = pandas_udf(cubed, returnType=LongType())\n",
    "\n",
    "# Create a Spark DataFrame, 'spark' is an existing SparkSession\n",
    "df = spark.range(1, 4)\n",
    "\n",
    "#  Execute function as a Spark vectorized UDF\n",
    "df.select(\"id\", cubed(col(\"id\"))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Chapter Five](https://learning.oreilly.com/library/view/learning-spark-2nd/9781492050032/ch05.html)\n",
    "> Higher order functions: TRANSFORM, FILTER, EXISTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|             celsius|\n",
      "+--------------------+\n",
      "|[35, 36, 32, 30, ...|\n",
      "|[31, 32, 34, 55, 56]|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = (SparkSession.builder.appName(\"SparkSQLExampleApp\").getOrCreate())\n",
    "\n",
    "#create dataframe\n",
    "schema = StructType([StructField(\"celsius\", ArrayType(IntegerType()))])\n",
    "\n",
    "t_list = [[35, 36, 32, 30, 40, 42, 38]], [[31, 32, 34, 55, 56]]\n",
    "t_c = spark.createDataFrame(t_list, schema)\n",
    "t_c.createOrReplaceTempView(\"tC\")\n",
    "\n",
    "# Show the DataFrame\n",
    "t_c.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|             celsius|          fahrenheit|\n",
      "+--------------------+--------------------+\n",
      "|[35, 36, 32, 30, ...|[95, 96, 89, 86, ...|\n",
      "|[31, 32, 34, 55, 56]|[87, 89, 93, 131,...|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate Fahrenheit from Celsius for an array of temperatures\n",
    "spark.sql(\n",
    "    \"\"\"\n",
    "SELECT celsius, \n",
    " transform(celsius, t -> ((t * 9) div 5) + 32) as fahrenheit \n",
    "  FROM tC\n",
    "\"\"\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+\n",
      "|             celsius|    high|\n",
      "+--------------------+--------+\n",
      "|[35, 36, 32, 30, ...|[40, 42]|\n",
      "|[31, 32, 34, 55, 56]|[55, 56]|\n",
      "+--------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Filter temperatures > 38C for array of temperatures\n",
    "spark.sql(\n",
    "    \"\"\"\n",
    "SELECT celsius, \n",
    " filter(celsius, t -> t > 38) as high \n",
    "  FROM tC\n",
    "\"\"\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+\n",
      "|             celsius|threshold|\n",
      "+--------------------+---------+\n",
      "|[35, 36, 32, 30, ...|     true|\n",
      "|[31, 32, 34, 55, 56]|    false|\n",
      "+--------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Is there a temperature of 38C in the array of temperatures\n",
    "spark.sql(\n",
    "    \"\"\"\n",
    "SELECT celsius, \n",
    "       exists(celsius, t -> t = 38) as threshold\n",
    "  FROM tC\n",
    "\"\"\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Chapter Five](https://learning.oreilly.com/library/view/learning-spark-2nd/9781492050032/ch05.html)\n",
    "> Common DataFrames and Spark SQL Operations: Union and Join "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "PARENT_DIR = os.popen('dirname $PWD').read().strip()\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = (SparkSession.builder.appName(\"SparkSQLExampleApp\").getOrCreate())\n",
    "\n",
    "#create dataframe\n",
    "\n",
    "# Set file paths\n",
    "from pyspark.sql.functions import expr\n",
    "\n",
    "tripdelaysFilePath = os.path.join(\n",
    "    PARENT_DIR,\n",
    "    \"databricks-datasets/learning-spark-v2/flights/departuredelays.csv\"\n",
    ")\n",
    "airportsnaFilePath = os.path.join(\n",
    "    PARENT_DIR,\n",
    "    \"databricks-datasets/learning-spark-v2/flights/airport-codes-na.txt\"\n",
    ")\n",
    "\n",
    "# Obtain airports data set\n",
    "airportsna = (\n",
    "    spark.read.format(\"csv\").options(\n",
    "        header=\"true\", inferSchema=\"true\", sep=\"\\t\"\n",
    "    ).load(airportsnaFilePath)\n",
    ")\n",
    "\n",
    "airportsna.createOrReplaceTempView(\"airports_na\")\n",
    "\n",
    "# Obtain departure delays data set\n",
    "departureDelays = (\n",
    "    spark.read.format(\"csv\").options(header=\"true\").load(tripdelaysFilePath)\n",
    ")\n",
    "\n",
    "departureDelays = (\n",
    "    departureDelays.withColumn(\"delay\",\n",
    "                               expr(\"CAST(delay as INT) as delay\")).withColumn(\n",
    "                                   \"distance\",\n",
    "                                   expr(\"CAST(distance as INT) as distance\")\n",
    "                               )\n",
    ")\n",
    "\n",
    "departureDelays.createOrReplaceTempView(\"departureDelays\")\n",
    "\n",
    "# Create temporary small table\n",
    "foo = (\n",
    "    departureDelays.filter(\n",
    "        expr(\n",
    "            \"\"\"origin == 'SEA' and destination == 'SFO' and \n",
    "    date like '01010%' and delay > 0\"\"\"\n",
    "        )\n",
    "    )\n",
    ")\n",
    "foo.createOrReplaceTempView(\"foo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+-------+----+\n",
      "|       City|State|Country|IATA|\n",
      "+-----------+-----+-------+----+\n",
      "| Abbotsford|   BC| Canada| YXX|\n",
      "|   Aberdeen|   SD|    USA| ABR|\n",
      "|    Abilene|   TX|    USA| ABI|\n",
      "|      Akron|   OH|    USA| CAK|\n",
      "|    Alamosa|   CO|    USA| ALS|\n",
      "|     Albany|   GA|    USA| ABY|\n",
      "|     Albany|   NY|    USA| ALB|\n",
      "|Albuquerque|   NM|    USA| ABQ|\n",
      "| Alexandria|   LA|    USA| AEX|\n",
      "|  Allentown|   PA|    USA| ABE|\n",
      "+-----------+-----+-------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM airports_na LIMIT 10\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+--------+------+-----------+\n",
      "|    date|delay|distance|origin|destination|\n",
      "+--------+-----+--------+------+-----------+\n",
      "|01011245|    6|     602|   ABE|        ATL|\n",
      "|01020600|   -8|     369|   ABE|        DTW|\n",
      "|01021245|   -2|     602|   ABE|        ATL|\n",
      "|01020605|   -4|     602|   ABE|        ATL|\n",
      "|01031245|   -4|     602|   ABE|        ATL|\n",
      "|01030605|    0|     602|   ABE|        ATL|\n",
      "|01041243|   10|     602|   ABE|        ATL|\n",
      "|01040605|   28|     602|   ABE|        ATL|\n",
      "|01051245|   88|     602|   ABE|        ATL|\n",
      "|01050605|    9|     602|   ABE|        ATL|\n",
      "+--------+-----+--------+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM departureDelays LIMIT 10\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+--------+------+-----------+\n",
      "|    date|delay|distance|origin|destination|\n",
      "+--------+-----+--------+------+-----------+\n",
      "|01010710|   31|     590|   SEA|        SFO|\n",
      "|01010955|  104|     590|   SEA|        SFO|\n",
      "|01010730|    5|     590|   SEA|        SFO|\n",
      "+--------+-----+--------+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM foo\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+--------+------+-----------+\n",
      "|    date|delay|distance|origin|destination|\n",
      "+--------+-----+--------+------+-----------+\n",
      "|01010710|   31|     590|   SEA|        SFO|\n",
      "|01010955|  104|     590|   SEA|        SFO|\n",
      "|01010730|    5|     590|   SEA|        SFO|\n",
      "|01010710|   31|     590|   SEA|        SFO|\n",
      "|01010955|  104|     590|   SEA|        SFO|\n",
      "|01010730|    5|     590|   SEA|        SFO|\n",
      "+--------+-----+--------+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Union two tables\n",
    "bar = departureDelays.union(foo)\n",
    "bar.createOrReplaceTempView(\"bar\")\n",
    "\n",
    "# Show the union (filtering for SEA and SFO in a specific time range)\n",
    "bar.filter(expr(\"\"\"origin == 'SEA' AND destination == 'SFO'\n",
    "AND date LIKE '01010%' AND delay > 0\"\"\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+--------+------+-----------+\n",
      "|    date|delay|distance|origin|destination|\n",
      "+--------+-----+--------+------+-----------+\n",
      "|01010710|   31|     590|   SEA|        SFO|\n",
      "|01010955|  104|     590|   SEA|        SFO|\n",
      "|01010730|    5|     590|   SEA|        SFO|\n",
      "|01010710|   31|     590|   SEA|        SFO|\n",
      "|01010955|  104|     590|   SEA|        SFO|\n",
      "|01010730|    5|     590|   SEA|        SFO|\n",
      "+--------+-----+--------+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Union in SQL\n",
    "spark.sql(\"\"\"\n",
    "SELECT * \n",
    "  FROM bar \n",
    " WHERE origin = 'SEA' \n",
    "   AND destination = 'SFO' \n",
    "   AND date LIKE '01010%' \n",
    "   AND delay > 0\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+--------+-----+--------+-----------+\n",
      "|   City|State|    date|delay|distance|destination|\n",
      "+-------+-----+--------+-----+--------+-----------+\n",
      "|Seattle|   WA|01010710|   31|     590|        SFO|\n",
      "|Seattle|   WA|01010955|  104|     590|        SFO|\n",
      "|Seattle|   WA|01010730|    5|     590|        SFO|\n",
      "+-------+-----+--------+-----+--------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Join in  SQL\n",
    "spark.sql(\"\"\"\n",
    "SELECT a.City, a.State, f.date, f.delay, f.distance, f.destination \n",
    "  FROM foo f\n",
    "  JOIN airports_na a\n",
    "    ON a.IATA = f.origin\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Chapter Five](https://learning.oreilly.com/library/view/learning-spark-2nd/9781492050032/ch05.html)\n",
    "> Common DataFrames and Spark SQL Operations: Windowing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import expr\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL Hive integration example\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "PARENT_DIR = os.popen('dirname $PWD').read().strip()\n",
    "\n",
    "\n",
    "# Set file paths\n",
    "\n",
    "tripdelaysFilePath = os.path.join(\n",
    "    PARENT_DIR,\n",
    "    \"databricks-datasets/learning-spark-v2/flights/departuredelays.csv\"\n",
    ")\n",
    "airportsnaFilePath = os.path.join(\n",
    "    PARENT_DIR,\n",
    "    \"databricks-datasets/learning-spark-v2/flights/airport-codes-na.txt\"\n",
    ")\n",
    "\n",
    "# Obtain airports data set\n",
    "airportsna = (\n",
    "    spark.read.format(\"csv\").options(\n",
    "        header=\"true\", inferSchema=\"true\", sep=\"\\t\"\n",
    "    ).load(airportsnaFilePath)\n",
    ")\n",
    "\n",
    "airportsna.createOrReplaceTempView(\"airports_na\")\n",
    "\n",
    "# Obtain departure delays data set\n",
    "departureDelays = (\n",
    "    spark.read.format(\"csv\").options(header=\"true\").load(tripdelaysFilePath)\n",
    ")\n",
    "\n",
    "departureDelays = (\n",
    "    departureDelays.withColumn(\"delay\",\n",
    "                               expr(\"CAST(delay as INT) as delay\")).withColumn(\n",
    "                                   \"distance\",\n",
    "                                   expr(\"CAST(distance as INT) as distance\")\n",
    "                               )\n",
    ")\n",
    "\n",
    "\n",
    "departureDelays.createOrReplaceTempView(\"departureDelays\")\n",
    "\n",
    "\n",
    "\n",
    "# Create temporary small table\n",
    "foo = (\n",
    "    departureDelays.filter(\n",
    "        expr(\n",
    "            \"\"\"origin == 'SEA' and destination == 'SFO' and \n",
    "    date like '01010%' and delay > 0\"\"\"\n",
    "        )\n",
    "    )\n",
    ")\n",
    "foo.createOrReplaceTempView(\"foo\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+-----------+\n",
      "|origin|destination|TotalDelays|\n",
      "+------+-----------+-----------+\n",
      "|   JFK|        ORD|       5608|\n",
      "|   SEA|        LAX|       9359|\n",
      "|   JFK|        SFO|      35619|\n",
      "|   SFO|        ORD|      27412|\n",
      "|   JFK|        DEN|       4315|\n",
      "|   SFO|        DEN|      18688|\n",
      "|   SFO|        SEA|      17080|\n",
      "|   SEA|        SFO|      22293|\n",
      "|   JFK|        ATL|      12141|\n",
      "|   SFO|        ATL|       5091|\n",
      "+------+-----------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Windowing : dense_rank\n",
    "\n",
    "spark.sql('DROP VIEW IF EXISTS departureDelaysWindow')\n",
    "\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "CREATE TEMP VIEW departureDelaysWindow  AS  \n",
    "SELECT origin, destination, SUM(delay) AS TotalDelays \n",
    "  FROM departureDelays \n",
    " WHERE origin IN ('SEA', 'SFO', 'JFK') \n",
    "   AND destination IN ('SEA', 'SFO', 'JFK', 'DEN', 'ORD', 'LAX', 'ATL') \n",
    " GROUP BY origin, destination\n",
    "\n",
    " \"\"\")\n",
    "\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "SELECT * FROM departureDelaysWindow  \n",
    " \"\"\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+-----------+\n",
      "|origin|destination|TotalDelays|\n",
      "+------+-----------+-----------+\n",
      "|   JFK|        LAX|      35755|\n",
      "|   JFK|        SFO|      35619|\n",
      "|   JFK|        ATL|      12141|\n",
      "+------+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "spark.sql(\"\"\"SELECT origin, destination, SUM(TotalDelays) AS TotalDelays\n",
    " FROM departureDelaysWindow\n",
    "WHERE origin = 'JFK'\n",
    "GROUP BY origin, destination\n",
    "ORDER BY SUM(TotalDelays) DESC\n",
    "LIMIT 3\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+-----------+----+\n",
      "|origin|destination|TotalDelays|rank|\n",
      "+------+-----------+-----------+----+\n",
      "|   SEA|        SFO|      22293|   1|\n",
      "|   SEA|        DEN|      13645|   2|\n",
      "|   SEA|        ORD|      10041|   3|\n",
      "|   SEA|        LAX|       9359|   4|\n",
      "|   SFO|        LAX|      40798|   1|\n",
      "|   SFO|        ORD|      27412|   2|\n",
      "|   SFO|        JFK|      24100|   3|\n",
      "|   SFO|        DEN|      18688|   4|\n",
      "|   JFK|        LAX|      35755|   1|\n",
      "|   JFK|        SFO|      35619|   2|\n",
      "|   JFK|        ATL|      12141|   3|\n",
      "|   JFK|        SEA|       7856|   4|\n",
      "+------+-----------+-----------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# dense_rank\n",
    " \n",
    "spark.sql(\"\"\"\n",
    "SELECT origin, destination, TotalDelays, rank \n",
    "  FROM ( \n",
    "     SELECT origin, destination, TotalDelays, dense_rank() \n",
    "       OVER (PARTITION BY origin ORDER BY TotalDelays DESC) as rank \n",
    "       FROM departureDelaysWindow\n",
    "  ) t \n",
    " WHERE rank <= 4\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+--------+------+-----------+-------+\n",
      "|    date|delay|distance|origin|destination| status|\n",
      "+--------+-----+--------+------+-----------+-------+\n",
      "|01010710|   31|     590|   SEA|        SFO|Delayed|\n",
      "|01010955|  104|     590|   SEA|        SFO|Delayed|\n",
      "|01010730|    5|     590|   SEA|        SFO|On-time|\n",
      "+--------+-----+--------+------+-----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#add column\n",
    "\n",
    "from pyspark.sql.functions import expr\n",
    "foo2 = (foo.withColumn(\n",
    "          \"status\", \n",
    "          expr(\"CASE WHEN delay <= 10 THEN 'On-time' ELSE 'Delayed' END\")\n",
    "        ))\n",
    "foo2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+------+-----------+-------+\n",
      "|    date|distance|origin|destination| status|\n",
      "+--------+--------+------+-----------+-------+\n",
      "|01010710|     590|   SEA|        SFO|Delayed|\n",
      "|01010955|     590|   SEA|        SFO|Delayed|\n",
      "|01010730|     590|   SEA|        SFO|On-time|\n",
      "+--------+--------+------+-----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# drop column\n",
    "foo3 = foo2.drop(\"delay\")\n",
    "foo3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+------+-----------+-------------+\n",
      "|    date|distance|origin|destination|flight_status|\n",
      "+--------+--------+------+-----------+-------------+\n",
      "|01010710|     590|   SEA|        SFO|      Delayed|\n",
      "|01010955|     590|   SEA|        SFO|      Delayed|\n",
      "|01010730|     590|   SEA|        SFO|      On-time|\n",
      "+--------+--------+------+-----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#renaming columns\n",
    "foo4 = foo3.withColumnRenamed(\"status\", \"flight_status\")\n",
    "foo4.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Chapter Five](https://learning.oreilly.com/library/view/learning-spark-2nd/9781492050032/ch05.html)\n",
    "> Common DataFrames and Spark SQL Operations: Pivoting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import expr\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL Hive integration example\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "PARENT_DIR = os.popen('dirname $PWD').read().strip()\n",
    "\n",
    "\n",
    "# Set file paths\n",
    "\n",
    "tripdelaysFilePath = os.path.join(\n",
    "    PARENT_DIR,\n",
    "    \"databricks-datasets/learning-spark-v2/flights/departuredelays.csv\"\n",
    ")\n",
    "airportsnaFilePath = os.path.join(\n",
    "    PARENT_DIR,\n",
    "    \"databricks-datasets/learning-spark-v2/flights/airport-codes-na.txt\"\n",
    ")\n",
    "\n",
    "# Obtain airports data set\n",
    "airportsna = (\n",
    "    spark.read.format(\"csv\").options(\n",
    "        header=\"true\", inferSchema=\"true\", sep=\"\\t\"\n",
    "    ).load(airportsnaFilePath)\n",
    ")\n",
    "\n",
    "airportsna.createOrReplaceTempView(\"airports_na\")\n",
    "\n",
    "# Obtain departure delays data set\n",
    "departureDelays = (\n",
    "    spark.read.format(\"csv\").options(header=\"true\").load(tripdelaysFilePath)\n",
    ")\n",
    "\n",
    "departureDelays = (\n",
    "    departureDelays.withColumn(\"delay\",\n",
    "                               expr(\"CAST(delay as INT) as delay\")).withColumn(\n",
    "                                   \"distance\",\n",
    "                                   expr(\"CAST(distance as INT) as distance\")\n",
    "                               )\n",
    ")\n",
    "\n",
    "\n",
    "departureDelays.createOrReplaceTempView(\"departureDelays\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[destination: string, month: int, delay: int]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT destination, CAST(SUBSTRING(date, 0, 2) AS int) AS month, delay \n",
    "  FROM departureDelays \n",
    " WHERE origin = 'SEA'\n",
    " \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+------------+------------+------------+\n",
      "|destination|JAN_AvgDelay|JAN_MaxDelay|FEB_AvgDelay|FEB_MaxDelay|\n",
      "+-----------+------------+------------+------------+------------+\n",
      "|        ABQ|       19.86|         316|       11.42|          69|\n",
      "|        ANC|        4.44|         149|        7.90|         141|\n",
      "|        ATL|       11.98|         397|        7.73|         145|\n",
      "|        AUS|        3.48|          50|       -0.21|          18|\n",
      "|        BOS|        7.84|         110|       14.58|         152|\n",
      "|        BUR|       -2.03|          56|       -1.89|          78|\n",
      "|        CLE|       16.00|          27|        null|        null|\n",
      "|        CLT|        2.53|          41|       12.96|         228|\n",
      "|        COS|        5.32|          82|       12.18|         203|\n",
      "|        CVG|       -0.50|           4|        null|        null|\n",
      "|        DCA|       -1.15|          50|        0.07|          34|\n",
      "|        DEN|       13.13|         425|       12.95|         625|\n",
      "|        DFW|        7.95|         247|       12.57|         356|\n",
      "|        DTW|        9.18|         107|        3.47|          77|\n",
      "|        EWR|        9.63|         236|        5.20|         212|\n",
      "|        FAI|        1.84|         160|        4.21|          60|\n",
      "|        FAT|        1.36|         119|        5.22|         232|\n",
      "|        FLL|        2.94|          54|        3.50|          40|\n",
      "|        GEG|        2.28|          63|        2.87|          60|\n",
      "|        HDN|       -0.44|          27|       -6.50|           0|\n",
      "+-----------+------------+------------+------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT * FROM (\n",
    "SELECT destination, CAST(SUBSTRING(date, 0, 2) AS int) AS month, delay \n",
    "  FROM departureDelays WHERE origin = 'SEA' \n",
    ") \n",
    "PIVOT (\n",
    "  CAST(AVG(delay) AS DECIMAL(4, 2)) AS AvgDelay, MAX(delay) AS MaxDelay\n",
    "  FOR month IN (1 JAN, 2 FEB)\n",
    ")\n",
    "ORDER BY destination\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
