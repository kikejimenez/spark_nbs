{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Learning Spark Second Edition](https://github.com/databricks/LearningSparkV2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " use _docker-compose.yml_ file located at he root directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Chapter 11](https://learning.oreilly.com/library/view/learning-spark-2nd/9781492050032/ch11.html)\n",
    "> MLflow: Track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Spark Session\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a SparkSession\n",
    "PARENT_DIR = os.popen('dirname $PWD').read().strip()\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Chapter 11. ML Flows\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# filepath\n",
    "filePath = 'databricks-datasets/learning-spark-v2/sf-airbnb/'+ \\\n",
    "          'sf-airbnb-clean.parquet/'\n",
    "filePath = os.path.join(PARENT_DIR,filePath)\n",
    "\n",
    "#load and create log price\n",
    "from pyspark.sql.functions import log\n",
    "airbnbDF = spark.read.parquet(filePath).withColumn('log_price', log('price'))\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "#train-test split\n",
    "trainDF, testDF = airbnbDF.randomSplit([.8, .2], seed=42)\n",
    "\n",
    "# Combine output of StringIndexer defined above and numeric columns\n",
    "from pyspark.ml.feature import VectorAssembler,StringIndexer\n",
    "\n",
    "categoricalCols = [field for (field, dataType) in trainDF.dtypes \n",
    "                   if dataType == \"string\"]\n",
    "\n",
    "indexOutputCols = [x + \"Index\" for x in categoricalCols]\n",
    "\n",
    "numericCols = [field for (field, dataType) in trainDF.dtypes \n",
    "               if ((dataType == \"double\") & (field != \"log_price\"))]\n",
    "\n",
    "stringIndexer = StringIndexer(inputCols=categoricalCols, \n",
    "                              outputCols=indexOutputCols, \n",
    "                              handleInvalid=\"skip\")\n",
    "\n",
    "assemblerInputs = indexOutputCols + numericCols\n",
    "vecAssembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\n",
    "\n",
    "from pyspark.ml.tuning import CrossValidator\n",
    "#Random Forest\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "rf = RandomForestRegressor(labelCol=\"log_price\", maxBins=40, seed=42)\n",
    "\n",
    "\n",
    "# pipeline\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "pipeline = Pipeline(stages = [stringIndexer, vecAssembler, rf])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Python \n",
    "import mlflow\n",
    "import mlflow.spark\n",
    "import pandas as pd\n",
    "\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "#from pyspark.ml.tuning import CrossValidator\n",
    "\n",
    "with mlflow.start_run(run_name=\"random-forest\") as run:\n",
    "    \n",
    "    # Log params: num_trees and max_depth\n",
    "    mlflow.log_param(\"num_trees\", rf.getNumTrees())\n",
    "    mlflow.log_param(\"max_depth\", rf.getMaxDepth())\n",
    "\n",
    "    # Log model\n",
    "    pipelineModel = pipeline.fit(trainDF)\n",
    "    mlflow.spark.log_model(pipelineModel, \"model\")\n",
    "\n",
    "    # Log metrics: RMSE and R2\n",
    "    predDF = pipelineModel.transform(testDF)\n",
    "    regressionEvaluator = RegressionEvaluator(predictionCol=\"prediction\", \n",
    "                                            labelCol=\"price\")\n",
    "    rmse = regressionEvaluator.setMetricName(\"rmse\").evaluate(predDF)\n",
    "    r2 = regressionEvaluator.setMetricName(\"r2\").evaluate(predDF)\n",
    "    mlflow.log_metrics({\"rmse\": rmse, \"r2\": r2})\n",
    "\n",
    "    # Log artifact: feature importance scores\n",
    "    rfModel = pipelineModel.stages[-1]\n",
    "    pandasDF = (pd.DataFrame(list(zip(vecAssembler.getInputCols(), \n",
    "                                    rfModel.featureImportances)), \n",
    "                           columns=[\"feature\", \"importance\"])\n",
    "              .sort_values(by=\"importance\", ascending=False))\n",
    "\n",
    "    # First write to local filesystem, then tell MLflow where to find that file\n",
    "    pandasDF.to_csv(\"feature-importance.csv\", index=False)\n",
    "    mlflow.log_artifact(\"feature-importance.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-07-28 21:09:59 +0000] [1538] [INFO] Starting gunicorn 20.0.4\n",
      "[2020-07-28 21:09:59 +0000] [1538] [ERROR] Connection in use: ('0.0.0.0', 5000)\n",
      "[2020-07-28 21:09:59 +0000] [1538] [ERROR] Retrying in 1 second.\n",
      "[2020-07-28 21:10:00 +0000] [1538] [ERROR] Connection in use: ('0.0.0.0', 5000)\n",
      "[2020-07-28 21:10:00 +0000] [1538] [ERROR] Retrying in 1 second.\n",
      "[2020-07-28 21:10:01 +0000] [1538] [ERROR] Connection in use: ('0.0.0.0', 5000)\n",
      "[2020-07-28 21:10:01 +0000] [1538] [ERROR] Retrying in 1 second.\n",
      "[2020-07-28 21:10:02 +0000] [1538] [ERROR] Connection in use: ('0.0.0.0', 5000)\n",
      "[2020-07-28 21:10:02 +0000] [1538] [ERROR] Retrying in 1 second.\n",
      "[2020-07-28 21:10:03 +0000] [1538] [ERROR] Connection in use: ('0.0.0.0', 5000)\n",
      "[2020-07-28 21:10:03 +0000] [1538] [ERROR] Retrying in 1 second.\n",
      "[2020-07-28 21:10:04 +0000] [1538] [ERROR] Can't connect to ('0.0.0.0', 5000)\n",
      "Running the mlflow server failed. Please see the logs above for details.\n"
     ]
    }
   ],
   "source": [
    "#!pip install mlflow > /dev/null\n",
    "#!mlflow server --host 0.0.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: mlflow server [OPTIONS]\r\n",
      "\r\n",
      "  Run the MLflow tracking server.\r\n",
      "\r\n",
      "  The server which listen on http://localhost:5000 by default, and only\r\n",
      "  accept connections from the local machine. To let the server accept\r\n",
      "  connections from other machines, you will need to pass --host 0.0.0.0 to\r\n",
      "  listen on all network interfaces (or a specific interface address).\r\n",
      "\r\n",
      "Options:\r\n",
      "  --backend-store-uri PATH     URI to which to persist experiment and run\r\n",
      "                               data. Acceptable URIs are SQLAlchemy-compatible\r\n",
      "                               database connection strings (e.g.\r\n",
      "                               'sqlite:///path/to/file.db') or local\r\n",
      "                               filesystem URIs (e.g.\r\n",
      "                               'file:///absolute/path/to/directory'). By\r\n",
      "                               default, data will be logged to the ./mlruns\r\n",
      "                               directory.\r\n",
      "\r\n",
      "  --default-artifact-root URI  Local or S3 URI to store artifacts, for new\r\n",
      "                               experiments. Note that this flag does not\r\n",
      "                               impact already-created experiments. Default:\r\n",
      "                               Within file store, if a file:/ URI is provided.\r\n",
      "                               If a sql backend is used, then this option is\r\n",
      "                               required.\r\n",
      "\r\n",
      "  -h, --host HOST              The network address to listen on (default:\r\n",
      "                               127.0.0.1). Use 0.0.0.0 to bind to all\r\n",
      "                               addresses if you want to access the tracking\r\n",
      "                               server from other machines.\r\n",
      "\r\n",
      "  -p, --port INTEGER           The port to listen on (default: 5000).\r\n",
      "  -w, --workers INTEGER        Number of gunicorn worker processes to handle\r\n",
      "                               requests (default: 4).\r\n",
      "\r\n",
      "  --static-prefix TEXT         A prefix which will be prepended to the path of\r\n",
      "                               all static paths.\r\n",
      "\r\n",
      "  --gunicorn-opts TEXT         Additional command line options forwarded to\r\n",
      "                               gunicorn processes.\r\n",
      "\r\n",
      "  --help                       Show this message and exit.\r\n"
     ]
    }
   ],
   "source": [
    "!mlflow server --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: mlflow ui [OPTIONS]\r\n",
      "Try 'mlflow ui --help' for help.\r\n",
      "\r\n",
      "Error: no such option: --host\r\n"
     ]
    }
   ],
   "source": [
    "#!mlflow ui "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Python\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "client = MlflowClient()\n",
    "runs = client.search_runs(run.info.experiment_id, \n",
    "                          order_by=[\"attributes.start_time desc\"], \n",
    "                          max_results=1)\n",
    "\n",
    "run_id = runs[0].info.run_id\n",
    "runs[0].data.metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
