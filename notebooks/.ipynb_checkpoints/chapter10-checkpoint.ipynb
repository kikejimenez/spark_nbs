{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Learning Spark Second Edition](https://github.com/databricks/LearningSparkV2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " _pyspark_ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Chapter 10](https://learning.oreilly.com/library/view/learning-spark-2nd/9781492050032/ch10.html)\n",
    "> Load Data, Train-Test Split, Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Spark Session\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "# Create a SparkSession\n",
    "\n",
    "\n",
    "PARENT_DIR = os.popen('dirname $PWD').read().strip()\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Chapter 10. Load. Train-Test Split. Linear Regression\") \\\n",
    "    .config(\"spark.sql.extensions\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+---------------+--------+---------+-----------------+-----+\n",
      "|neighbourhood_cleansed|      room_type|bedrooms|bathrooms|number_of_reviews|price|\n",
      "+----------------------+---------------+--------+---------+-----------------+-----+\n",
      "|      Western Addition|Entire home/apt|     1.0|      1.0|            180.0|170.0|\n",
      "|        Bernal Heights|Entire home/apt|     2.0|      1.0|            111.0|235.0|\n",
      "|        Haight Ashbury|   Private room|     1.0|      4.0|             17.0| 65.0|\n",
      "|        Haight Ashbury|   Private room|     1.0|      4.0|              8.0| 65.0|\n",
      "|      Western Addition|Entire home/apt|     2.0|      1.5|             27.0|785.0|\n",
      "+----------------------+---------------+--------+---------+-----------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load Data\n",
    "filePath = 'databricks-datasets/learning-spark-v2/sf-airbnb/'+ \\\n",
    "          'sf-airbnb-clean.parquet/'\n",
    "filePath = os.path.join(PARENT_DIR,filePath)\n",
    "\n",
    "airbnbDF = spark.read.parquet(filePath)\n",
    "airbnbDF.select(\"neighbourhood_cleansed\", \"room_type\", \"bedrooms\", \"bathrooms\", \n",
    "                \"number_of_reviews\", \"price\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 5780 rows in the training set, \n",
      "          and 1366 in the test set\n"
     ]
    }
   ],
   "source": [
    "# Train Test Split\n",
    "trainDF, testDF = airbnbDF.randomSplit([.8, .2], seed=42)\n",
    "print(f\"\"\"There are {trainDF.count()} rows in the training set, \n",
    "          and {testDF.count()} in the test set\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+-----+\n",
      "|bedrooms|features|price|\n",
      "+--------+--------+-----+\n",
      "|     1.0|   [1.0]|200.0|\n",
      "|     1.0|   [1.0]|130.0|\n",
      "|     1.0|   [1.0]| 95.0|\n",
      "|     1.0|   [1.0]|250.0|\n",
      "|     3.0|   [3.0]|250.0|\n",
      "|     1.0|   [1.0]|115.0|\n",
      "|     1.0|   [1.0]|105.0|\n",
      "|     1.0|   [1.0]| 86.0|\n",
      "|     1.0|   [1.0]|100.0|\n",
      "|     2.0|   [2.0]|220.0|\n",
      "+--------+--------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Transformers\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "vecAssembler = VectorAssembler(inputCols=[\"bedrooms\"], outputCol=\"features\")\n",
    "vecTrainDF = vecAssembler.transform(trainDF)\n",
    "vecTrainDF.select(\"bedrooms\", \"features\", \"price\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Linear Regression\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"price\")\n",
    "lrModel = lr.fit(vecTrainDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The formula for the linear regression line is \n",
      "price = 123.68*bedrooms + 47.51\n"
     ]
    }
   ],
   "source": [
    "# Regression coefficients\n",
    "m = round(lrModel.coefficients[0], 2)\n",
    "b = round(lrModel.intercept, 2)\n",
    "\n",
    "print(f\"\"\"The formula for the linear regression line is \n",
    "price = {m}*bedrooms + {b}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Chapter 10](https://learning.oreilly.com/library/view/learning-spark-2nd/9781492050032/ch10.html)\n",
    "> Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Spark Session\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "# Create a SparkSession\n",
    "\n",
    "\n",
    "PARENT_DIR = os.popen('dirname $PWD').read().strip()\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Chapter 10. Pipeline\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "# Load Data\n",
    "filePath = 'databricks-datasets/learning-spark-v2/sf-airbnb/'+ \\\n",
    "          'sf-airbnb-clean.parquet/'\n",
    "filePath = os.path.join(PARENT_DIR,filePath)\n",
    "\n",
    "#train-test split\n",
    "airbnbDF = spark.read.parquet(filePath)\n",
    "trainDF, testDF = airbnbDF.randomSplit([.8, .2], seed=42)\n",
    "\n",
    "\n",
    "# Transformers\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "vecAssembler = VectorAssembler(inputCols=[\"bedrooms\"], outputCol=\"features\")\n",
    "vecTrainDF = vecAssembler.transform(trainDF)\n",
    "\n",
    "#  Linear Regression\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"price\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline Model\n",
    "from pyspark.ml import Pipeline\n",
    "pipeline = Pipeline(stages=[vecAssembler, lr])\n",
    "pipelineModel = pipeline.fit(trainDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+------+------------------+\n",
      "|bedrooms|features| price|        prediction|\n",
      "+--------+--------+------+------------------+\n",
      "|     1.0|   [1.0]|  85.0|171.18598011578285|\n",
      "|     1.0|   [1.0]|  45.0|171.18598011578285|\n",
      "|     1.0|   [1.0]|  70.0|171.18598011578285|\n",
      "|     1.0|   [1.0]| 128.0|171.18598011578285|\n",
      "|     1.0|   [1.0]| 159.0|171.18598011578285|\n",
      "|     2.0|   [2.0]| 250.0|294.86172649777757|\n",
      "|     1.0|   [1.0]|  99.0|171.18598011578285|\n",
      "|     1.0|   [1.0]|  95.0|171.18598011578285|\n",
      "|     1.0|   [1.0]| 100.0|171.18598011578285|\n",
      "|     1.0|   [1.0]|2010.0|171.18598011578285|\n",
      "+--------+--------+------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Predict\n",
    "predDF = pipelineModel.transform(testDF)\n",
    "predDF.select(\"bedrooms\", \"features\", \"price\", \"prediction\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Chapter 10](https://learning.oreilly.com/library/view/learning-spark-2nd/9781492050032/ch10.html)\n",
    "> One-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Spark Session\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "# Create a SparkSession\n",
    "\n",
    "\n",
    "PARENT_DIR = os.popen('dirname $PWD').read().strip()\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Chapter 10. One hot Encoders\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "# Load Data\n",
    "filePath = 'databricks-datasets/learning-spark-v2/sf-airbnb/'+ \\\n",
    "          'sf-airbnb-clean.parquet/'\n",
    "filePath = os.path.join(PARENT_DIR,filePath)\n",
    "\n",
    "#train-test split\n",
    "airbnbDF = spark.read.parquet(filePath)\n",
    "trainDF, testDF = airbnbDF.randomSplit([.8, .2], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "\n",
    "categoricalCols = [field for (field, dataType) in trainDF.dtypes \n",
    "                   if dataType == \"string\"]\n",
    "\n",
    "indexOutputCols = [x + \"Index\" for x in categoricalCols]\n",
    "oheOutputCols = [x + \"OHE\" for x in categoricalCols]\n",
    "\n",
    "stringIndexer = StringIndexer(inputCols=categoricalCols, \n",
    "                              outputCols=indexOutputCols, \n",
    "                              handleInvalid=\"skip\")\n",
    "\n",
    "oheEncoder = OneHotEncoder(inputCols=indexOutputCols, \n",
    "                           outputCols=oheOutputCols)\n",
    "\n",
    "numericCols = [field for (field, dataType) in trainDF.dtypes \n",
    "               if ((dataType == \"double\") & (field != \"price\"))]\n",
    "\n",
    "assemblerInputs = oheOutputCols + numericCols\n",
    "vecAssembler = VectorAssembler(inputCols=assemblerInputs, \n",
    "                               outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels to be included using R programming Language Sintaxis\n",
    "from pyspark.ml.feature import RFormula\n",
    "\n",
    "rFormula = RFormula(formula=\"price ~ .\", \n",
    "                    featuresCol=\"features\", \n",
    "                    labelCol=\"price\", \n",
    "                    handleInvalid=\"skip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+------------------+\n",
      "|            features|price|        prediction|\n",
      "+--------------------+-----+------------------+\n",
      "|(98,[0,3,6,22,43,...| 85.0| 55.24365707389188|\n",
      "|(98,[0,3,6,22,43,...| 45.0|23.357685914717877|\n",
      "|(98,[0,3,6,22,43,...| 70.0|28.474464479034395|\n",
      "|(98,[0,3,6,12,42,...|128.0| -91.6079079594947|\n",
      "|(98,[0,3,6,12,43,...|159.0| 95.05688229945372|\n",
      "+--------------------+-----+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#  Linear Regression\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"price\")\n",
    "\n",
    "# pipeline Model\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "pipeline = Pipeline(stages = [stringIndexer, oheEncoder, vecAssembler, lr])\n",
    "pipelineModel = pipeline.fit(trainDF)\n",
    "\n",
    "#predict\n",
    "predDF = pipelineModel.transform(testDF)\n",
    "predDF.select(\"features\", \"price\", \"prediction\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Chapter 10](https://learning.oreilly.com/library/view/learning-spark-2nd/9781492050032/ch10.html)\n",
    "> Evaluating Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Spark Session\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "# Create a SparkSession\n",
    "\n",
    "\n",
    "PARENT_DIR = os.popen('dirname $PWD').read().strip()\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Chapter 10. One hot Encoders\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "# Load Data\n",
    "filePath = 'databricks-datasets/learning-spark-v2/sf-airbnb/'+ \\\n",
    "          'sf-airbnb-clean.parquet/'\n",
    "filePath = os.path.join(PARENT_DIR,filePath)\n",
    "\n",
    "#train-test split\n",
    "airbnbDF = spark.read.parquet(filePath)\n",
    "trainDF, testDF = airbnbDF.randomSplit([.8, .2], seed=42)\n",
    "\n",
    "#  Linear Regression\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"price\")\n",
    "\n",
    "#one-hot encoding\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "\n",
    "categoricalCols = [field for (field, dataType) in trainDF.dtypes \n",
    "                   if dataType == \"string\"]\n",
    "indexOutputCols = [x + \"Index\" for x in categoricalCols]\n",
    "oheOutputCols = [x + \"OHE\" for x in categoricalCols]\n",
    "stringIndexer = StringIndexer(inputCols=categoricalCols, \n",
    "                              outputCols=indexOutputCols, \n",
    "                              handleInvalid=\"skip\")\n",
    "oheEncoder = OneHotEncoder(inputCols=indexOutputCols, \n",
    "                           outputCols=oheOutputCols)\n",
    "numericCols = [field for (field, dataType) in trainDF.dtypes \n",
    "               if ((dataType == \"double\") & (field != \"price\"))]\n",
    "assemblerInputs = oheOutputCols + numericCols\n",
    "vecAssembler = VectorAssembler(inputCols=assemblerInputs, \n",
    "                               outputCol=\"features\")\n",
    "\n",
    "\n",
    "# pipeline Model\n",
    "from pyspark.ml import Pipeline\n",
    "pipeline = Pipeline(stages = [stringIndexer, oheEncoder, vecAssembler, lr])\n",
    "pipelineModel = pipeline.fit(trainDF)\n",
    "\n",
    "#predict\n",
    "predDF = pipelineModel.transform(testDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE is 220.6\n"
     ]
    }
   ],
   "source": [
    "# Root Mean Squared Error (RMSE)\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "regressionEvaluator = RegressionEvaluator(\n",
    "  predictionCol=\"prediction\", \n",
    "  labelCol=\"price\", \n",
    "  metricName=\"rmse\")\n",
    "rmse = regressionEvaluator.evaluate(predDF)\n",
    "print(f\"RMSE is {rmse:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 is 0.16043316698848087\n"
     ]
    }
   ],
   "source": [
    "# R squared\n",
    "r2 = regressionEvaluator.setMetricName(\"r2\").evaluate(predDF)\n",
    "print(f\"R2 is {r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Chapter 10](https://learning.oreilly.com/library/view/learning-spark-2nd/9781492050032/ch10.html)\n",
    "> Linear Regression Baseline RMSE Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE is 240.8\n"
     ]
    }
   ],
   "source": [
    "# Start Spark Session\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a SparkSession\n",
    "PARENT_DIR = os.popen('dirname $PWD').read().strip()\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Chapter 10. One hot Encoders\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load Data\n",
    "filePath = 'databricks-datasets/learning-spark-v2/sf-airbnb/'+ \\\n",
    "          'sf-airbnb-clean.parquet/'\n",
    "filePath = os.path.join(PARENT_DIR,filePath)\n",
    "\n",
    "#train-test split\n",
    "airbnbDF = spark.read.parquet(filePath)\n",
    "trainDF, testDF = airbnbDF.randomSplit([.8, .2], seed=42)\n",
    "\n",
    "#new constant column\n",
    "from pyspark.sql.functions import lit\n",
    "mean_price = trainDF.agg({'price':'mean'}).collect()[0][0]\n",
    "regDF = trainDF.withColumn('mean_price', lit(mean_price))\n",
    "\n",
    "\n",
    "#one-hot encoding\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "\n",
    "categoricalCols = [field for (field, dataType) in regDF.dtypes \n",
    "                   if dataType == \"string\"]\n",
    "indexOutputCols = [x + \"Index\" for x in categoricalCols]\n",
    "oheOutputCols = [x + \"OHE\" for x in categoricalCols]\n",
    "stringIndexer = StringIndexer(inputCols=categoricalCols, \n",
    "                              outputCols=indexOutputCols, \n",
    "                              handleInvalid=\"skip\")\n",
    "oheEncoder = OneHotEncoder(inputCols=indexOutputCols, \n",
    "                           outputCols=oheOutputCols)\n",
    "numericCols = [field for (field, dataType) in regDF.dtypes \n",
    "               if ((dataType == \"double\") & (field != \"mean_price\"))]\n",
    "assemblerInputs = oheOutputCols + numericCols\n",
    "vecAssembler = VectorAssembler(inputCols=assemblerInputs, \n",
    "                               outputCol=\"features\")\n",
    "\n",
    "\n",
    "#  Linear Regression\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"mean_price\")\n",
    "\n",
    "# pipeline Model\n",
    "from pyspark.ml import Pipeline\n",
    "pipeline = Pipeline(stages = [stringIndexer, oheEncoder, vecAssembler, lr])\n",
    "pipelineModel = pipeline.fit(regDF)\n",
    "\n",
    "#predict\n",
    "predDF = pipelineModel.transform(testDF)\n",
    "\n",
    "#evaluations\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "regressionEvaluator = RegressionEvaluator(\n",
    "  predictionCol=\"prediction\", \n",
    "  labelCol=\"price\", \n",
    "  metricName=\"rmse\")\n",
    "rmse = regressionEvaluator.evaluate(predDF)\n",
    "print(f\"RMSE is {rmse:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The horizontal line \n",
      "price = 0.0*bedrooms + 214.47\n"
     ]
    }
   ],
   "source": [
    "# Regression coefficients\n",
    "model = pipelineModel.stages[-1]\n",
    "m = round(model.coefficients[0], 2)\n",
    "b = round(model.intercept, 2)\n",
    "\n",
    "print(f\"\"\"The horizontal line \n",
    "price = {m}*bedrooms + {b}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Chapter 10](https://learning.oreilly.com/library/view/learning-spark-2nd/9781492050032/ch10.html)\n",
    "> Logarithm of Price Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE is 0.3\n"
     ]
    }
   ],
   "source": [
    "# Start Spark Session\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a SparkSession\n",
    "PARENT_DIR = os.popen('dirname $PWD').read().strip()\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Chapter 10. One hot Encoders\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load Data\n",
    "filePath = 'databricks-datasets/learning-spark-v2/sf-airbnb/'+ \\\n",
    "          'sf-airbnb-clean.parquet/'\n",
    "filePath = os.path.join(PARENT_DIR,filePath)\n",
    "\n",
    "#log price\n",
    "\n",
    "from pyspark.sql.functions import log\n",
    "airbnbDF = spark.read.parquet(filePath)\n",
    "airbnbDF = airbnbDF.withColumn('log_price', log('price'))\n",
    "\n",
    "\n",
    "#train-test split\n",
    "trainDF, testDF = airbnbDF.randomSplit([.8, .2], seed=42)\n",
    "\n",
    "\n",
    "#one-hot encoding\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "\n",
    "categoricalCols = [field for (field, dataType) in trainDF.dtypes \n",
    "                   if dataType == \"string\"]\n",
    "indexOutputCols = [x + \"Index\" for x in categoricalCols]\n",
    "oheOutputCols = [x + \"OHE\" for x in categoricalCols]\n",
    "stringIndexer = StringIndexer(inputCols=categoricalCols, \n",
    "                              outputCols=indexOutputCols, \n",
    "                              handleInvalid=\"skip\")\n",
    "oheEncoder = OneHotEncoder(inputCols=indexOutputCols, \n",
    "                           outputCols=oheOutputCols)\n",
    "numericCols = [field for (field, dataType) in trainDF.dtypes \n",
    "               if ((dataType == \"double\") & (field != \"log_price\"))]\n",
    "assemblerInputs = oheOutputCols + numericCols\n",
    "vecAssembler = VectorAssembler(inputCols=assemblerInputs, \n",
    "                               outputCol=\"features\")\n",
    "\n",
    "\n",
    "#  Linear Regression\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"log_price\")\n",
    "\n",
    "# pipeline Model\n",
    "from pyspark.ml import Pipeline\n",
    "pipeline = Pipeline(stages = [stringIndexer, oheEncoder, vecAssembler, lr])\n",
    "pipelineModel = pipeline.fit(trainDF)\n",
    "\n",
    "#predict\n",
    "predDF = pipelineModel.transform(testDF)\n",
    "\n",
    "#evaluations\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "regressionEvaluator = RegressionEvaluator(\n",
    "  predictionCol=\"prediction\", \n",
    "  labelCol=\"log_price\", \n",
    "  metricName=\"rmse\")\n",
    "rmse = regressionEvaluator.evaluate(predDF)\n",
    "print(f\"RMSE is {rmse:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 is 0.7668615633102807\n"
     ]
    }
   ],
   "source": [
    "# R squared\n",
    "r2 = regressionEvaluator.setMetricName(\"r2\").evaluate(predDF)\n",
    "print(f\"R2 is {r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save Model\n",
    "pipelinePath = \"/tmp/lr-pipeline-model\"\n",
    "pipelineModel.write().overwrite().save(pipelinePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Model\n",
    "from pyspark.ml import PipelineModel\n",
    "savedPipelineModel = PipelineModel.load(pipelinePath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Chapter 10](https://learning.oreilly.com/library/view/learning-spark-2nd/9781492050032/ch10.html)\n",
    "> Hyperparameter Tuning: Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Spark Session\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a SparkSession\n",
    "PARENT_DIR = os.popen('dirname $PWD').read().strip()\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Chapter 10. Decision Tree\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# filepath\n",
    "filePath = 'databricks-datasets/learning-spark-v2/sf-airbnb/'+ \\\n",
    "          'sf-airbnb-clean.parquet/'\n",
    "filePath = os.path.join(PARENT_DIR,filePath)\n",
    "\n",
    "#load and create log price\n",
    "from pyspark.sql.functions import log\n",
    "airbnbDF = spark.read.parquet(filePath).withColumn('log_price', log('price'))\n",
    "\n",
    "#train-test split\n",
    "trainDF, testDF = airbnbDF.randomSplit([.8, .2], seed=42)\n",
    "\n",
    "# Combine output of StringIndexer defined above and numeric columns\n",
    "from pyspark.ml.feature import VectorAssembler,StringIndexer\n",
    "\n",
    "categoricalCols = [field for (field, dataType) in trainDF.dtypes \n",
    "                   if dataType == \"string\"]\n",
    "\n",
    "indexOutputCols = [x + \"Index\" for x in categoricalCols]\n",
    "\n",
    "numericCols = [field for (field, dataType) in trainDF.dtypes \n",
    "               if ((dataType == \"double\") & (field != \"log_price\"))]\n",
    "\n",
    "stringIndexer = StringIndexer(inputCols=categoricalCols, \n",
    "                              outputCols=indexOutputCols, \n",
    "                              handleInvalid=\"skip\")\n",
    "\n",
    "assemblerInputs = indexOutputCols + numericCols\n",
    "vecAssembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\n",
    "\n",
    "#decision tree\n",
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "dt = DecisionTreeRegressor(labelCol=\"log_price\")\n",
    "dt.setMaxBins(40)\n",
    "\n",
    "# Combine stages into pipeline\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "stages = [stringIndexer, vecAssembler, dt]\n",
    "pipeline = Pipeline(stages=stages)\n",
    "pipelineModel = pipeline.fit(trainDF) # This line should error\n",
    "\n",
    "\n",
    "#predict\n",
    "predDF = pipelineModel.transform(testDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeRegressionModel: uid=DecisionTreeRegressor_a0f85d78f61d, depth=5, numNodes=63, numFeatures=34\n",
      "  If (feature 23 <= 178.5)\n",
      "   If (feature 23 <= 90.5)\n",
      "    If (feature 23 <= 54.5)\n",
      "     If (feature 23 <= 45.5)\n",
      "      If (feature 3 in {1.0,3.0,4.0,6.0,24.0})\n",
      "       Predict: 3.34800174250524\n",
      "      Else (feature 3 not in {1.0,3.0,4.0,6.0,24.0})\n",
      "       Predict: 3.6828292005441248\n",
      "     Else (feature 23 > 45.5)\n",
      "      If (feature 3 in {0.0,1.0,2.0,3.0,5.0,6.0,8.0,10.0,11.0,13.0,15.0,16.0,17.0,18....\n"
     ]
    }
   ],
   "source": [
    "# tree in if-else form\n",
    "dtModel = pipelineModel.stages[-1]\n",
    "print(dtModel.toDebugString[:500]+'...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>price</td>\n",
       "      <td>0.995259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>neighbourhood_cleansedIndex</td>\n",
       "      <td>0.002481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>latitude</td>\n",
       "      <td>0.001943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>review_scores_location</td>\n",
       "      <td>0.000288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cancellation_policyIndex</td>\n",
       "      <td>0.000028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>beds_na</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>review_scores_communication</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        feature  importance\n",
       "23                        price    0.995259\n",
       "3   neighbourhood_cleansedIndex    0.002481\n",
       "8                      latitude    0.001943\n",
       "21       review_scores_location    0.000288\n",
       "1      cancellation_policyIndex    0.000028\n",
       "26                      beds_na    0.000000\n",
       "20  review_scores_communication    0.000000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# feature importance\n",
    "import pandas as pd\n",
    "\n",
    "featureImp = pd.DataFrame(\n",
    "  list(zip(vecAssembler.getInputCols(), dtModel.featureImportances)),\n",
    "  columns=[\"feature\", \"importance\"])\n",
    "featureImp.sort_values(by=\"importance\", ascending=False).head(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Chapter 10](https://learning.oreilly.com/library/view/learning-spark-2nd/9781492050032/ch10.html)\n",
    "> Random Forest & Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Spark Session\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a SparkSession\n",
    "PARENT_DIR = os.popen('dirname $PWD').read().strip()\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Chapter 10. Random Forest\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# filepath\n",
    "filePath = 'databricks-datasets/learning-spark-v2/sf-airbnb/'+ \\\n",
    "          'sf-airbnb-clean.parquet/'\n",
    "filePath = os.path.join(PARENT_DIR,filePath)\n",
    "\n",
    "#load and create log price\n",
    "from pyspark.sql.functions import log\n",
    "airbnbDF = spark.read.parquet(filePath).withColumn('log_price', log('price'))\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "#train-test split\n",
    "trainDF, testDF = airbnbDF.randomSplit([.8, .2], seed=42)\n",
    "\n",
    "# Combine output of StringIndexer defined above and numeric columns\n",
    "from pyspark.ml.feature import VectorAssembler,StringIndexer\n",
    "\n",
    "categoricalCols = [field for (field, dataType) in trainDF.dtypes \n",
    "                   if dataType == \"string\"]\n",
    "\n",
    "indexOutputCols = [x + \"Index\" for x in categoricalCols]\n",
    "\n",
    "numericCols = [field for (field, dataType) in trainDF.dtypes \n",
    "               if ((dataType == \"double\") & (field != \"log_price\"))]\n",
    "\n",
    "stringIndexer = StringIndexer(inputCols=categoricalCols, \n",
    "                              outputCols=indexOutputCols, \n",
    "                              handleInvalid=\"skip\")\n",
    "\n",
    "assemblerInputs = indexOutputCols + numericCols\n",
    "vecAssembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\n",
    "\n",
    "#Random Forest\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "rf = RandomForestRegressor(labelCol=\"log_price\", maxBins=40, seed=42)\n",
    "\n",
    "\n",
    "# pipeline\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "pipeline = Pipeline(stages = [stringIndexer, vecAssembler, rf])\n",
    "\n",
    "#gridbuilder\n",
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "paramGrid = (ParamGridBuilder()\n",
    "            .addGrid(rf.maxDepth, [2, 4, 6])\n",
    "            .addGrid(rf.numTrees, [10, 100])\n",
    "            .build())\n",
    "\n",
    "#evaluator\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "evaluator = RegressionEvaluator(labelCol=\"log_price\", \n",
    "                                predictionCol=\"prediction\", \n",
    "                                metricName=\"rmse\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.66 s, sys: 390 ms, total: 2.05 s\n",
      "Wall time: 41.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Cross Validation\n",
    "\n",
    "from pyspark.ml.tuning import CrossValidator\n",
    "\n",
    "cv = CrossValidator(estimator=pipeline, \n",
    "                    evaluator=evaluator, \n",
    "                    estimatorParamMaps=paramGrid, \n",
    "                    numFolds=3, \n",
    "                    seed=42)\n",
    "cvModel = cv.fit(trainDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[({Param(parent='RandomForestRegressor_64d6751757f0', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.'): 2,\n",
       "   Param(parent='RandomForestRegressor_64d6751757f0', name='numTrees', doc='Number of trees to train (>= 1).'): 10},\n",
       "  0.3172766328923306),\n",
       " ({Param(parent='RandomForestRegressor_64d6751757f0', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.'): 2,\n",
       "   Param(parent='RandomForestRegressor_64d6751757f0', name='numTrees', doc='Number of trees to train (>= 1).'): 100},\n",
       "  0.3358904413789592),\n",
       " ({Param(parent='RandomForestRegressor_64d6751757f0', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.'): 4,\n",
       "   Param(parent='RandomForestRegressor_64d6751757f0', name='numTrees', doc='Number of trees to train (>= 1).'): 10},\n",
       "  0.18139680049708926),\n",
       " ({Param(parent='RandomForestRegressor_64d6751757f0', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.'): 4,\n",
       "   Param(parent='RandomForestRegressor_64d6751757f0', name='numTrees', doc='Number of trees to train (>= 1).'): 100},\n",
       "  0.20991204050271395),\n",
       " ({Param(parent='RandomForestRegressor_64d6751757f0', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.'): 6,\n",
       "   Param(parent='RandomForestRegressor_64d6751757f0', name='numTrees', doc='Number of trees to train (>= 1).'): 10},\n",
       "  0.13514530418584536),\n",
       " ({Param(parent='RandomForestRegressor_64d6751757f0', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.'): 6,\n",
       "   Param(parent='RandomForestRegressor_64d6751757f0', name='numTrees', doc='Number of trees to train (>= 1).'): 100},\n",
       "  0.14541953351035444)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In Python\n",
    "list(zip(cvModel.getEstimatorParamMaps(), cvModel.avgMetrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.86 s, sys: 919 ms, total: 3.78 s\n",
      "Wall time: 32 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Validate in Parallel\n",
    "cvModel = cv.setParallelism(4).fit(trainDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1e+03 ms, sys: 314 ms, total: 1.31 s\n",
      "Wall time: 22.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# put cross-validator inside pipeline (instead of pipeline inside cross-validator)\n",
    "\n",
    "cv = CrossValidator(estimator=rf, \n",
    "                    evaluator=evaluator, \n",
    "                    estimatorParamMaps=paramGrid, \n",
    "                    numFolds=3, \n",
    "                    parallelism=4, \n",
    "                    seed=42)\n",
    "\n",
    "pipeline = Pipeline(stages=[stringIndexer, vecAssembler, cv])\n",
    "pipelineModel = pipeline.fit(trainDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
