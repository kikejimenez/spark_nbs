{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Learning Spark Second Edition](https://github.com/databricks/LearningSparkV2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " _pyspark_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Python\n",
    "# Configure source data path\n",
    "\n",
    "# In Python\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "# Create a SparkSession\n",
    "\n",
    "\n",
    "\n",
    "#os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages io.delta:delta-core_2.12:0.7.0 pyspark-shell'\n",
    "\n",
    "PARENT_DIR = os.popen('dirname $PWD').read().strip()\n",
    "\n",
    "spark = SparkSession.builder.appName(\"MyApp\") \\\n",
    "    .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:0.7.0\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sourcePath = os.path.join(\n",
    "    PARENT_DIR,\n",
    "    'databricks-datasets/learning-spark-v2/loans/'+\\\n",
    "              'loan-risks.snappy.parquet'\n",
    ")\n",
    "\n",
    "# Configure Delta Lake path\n",
    "deltaPath = os.path.join(\"/tmp\",\"loans_delta\")\n",
    "\n",
    "# Create the Delta Lake table with the same loans data\n",
    "parquet = (spark.read.format(\"parquet\")\n",
    "           .load(sourcePath)\n",
    "           .write\n",
    "           .mode('overwrite')\n",
    "           .format(\"delta\")\n",
    "           .save(deltaPath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shutil import rmtree\n",
    "assert os.path.isdir(deltaPath)\n",
    "rmtree(deltaPath,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a view on the data called loans_delta\n",
    "spark.read.format(\"delta\").load(deltaPath).createOrReplaceTempView(\"loans_delta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                   // Or use any other 2.x version here\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql._\u001b[39m"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`org.apache.spark::spark-sql:3.0.0` // Or use any other 2.x version here\n",
    "import org.apache.spark.sql._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.log4j.{Level, Logger}\n",
       "\u001b[39m"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.log4j.{Level, Logger}\n",
    "Logger.getLogger(\"org\").setLevel(Level.OFF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Chapter 9](https://learning.oreilly.com/library/view/Learning+Spark,+2nd+Edition/9781492050032/ch07.html#executor_memory_layout)\n",
    "> Optimizing and Tuning Spark for Efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.SparkSession\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[36mspark\u001b[39m: \u001b[32mSparkSession\u001b[39m = org.apache.spark.sql.SparkSession@4e718e42"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "\n",
    "val spark = SparkSession\n",
    "  .builder()\n",
    "  .appName(\"Chapter 9\")\n",
    "  .master(\"local[*]\")\n",
    "  .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "  .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "  .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31mjava.lang.ClassNotFoundException: Failed to find data source: delta. Please find packages at http://spark.apache.org/third-party-projects.html\u001b[39m\n  org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(\u001b[32mDataSource.scala\u001b[39m:\u001b[32m674\u001b[39m)\n  org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(\u001b[32mDataSource.scala\u001b[39m:\u001b[32m728\u001b[39m)\n  org.apache.spark.sql.DataFrameWriter.lookupV2Provider(\u001b[32mDataFrameWriter.scala\u001b[39m:\u001b[32m948\u001b[39m)\n  org.apache.spark.sql.DataFrameWriter.save(\u001b[32mDataFrameWriter.scala\u001b[39m:\u001b[32m285\u001b[39m)\n  org.apache.spark.sql.DataFrameWriter.save(\u001b[32mDataFrameWriter.scala\u001b[39m:\u001b[32m269\u001b[39m)\n  ammonite.$sess.cmd8$Helper.<init>(\u001b[32mcmd8.sc\u001b[39m:\u001b[32m13\u001b[39m)\n  ammonite.$sess.cmd8$.<init>(\u001b[32mcmd8.sc\u001b[39m:\u001b[32m7\u001b[39m)\n  ammonite.$sess.cmd8$.<clinit>(\u001b[32mcmd8.sc\u001b[39m:\u001b[32m-1\u001b[39m)\n\u001b[31mjava.lang.ClassNotFoundException: delta.DefaultSource\u001b[39m\n  java.net.URLClassLoader.findClass(\u001b[32mURLClassLoader.java\u001b[39m:\u001b[32m382\u001b[39m)\n  ammonite.runtime.SpecialClassLoader.findClass(\u001b[32mClassLoaders.scala\u001b[39m:\u001b[32m241\u001b[39m)\n  java.lang.ClassLoader.loadClass(\u001b[32mClassLoader.java\u001b[39m:\u001b[32m418\u001b[39m)\n  java.lang.ClassLoader.loadClass(\u001b[32mClassLoader.java\u001b[39m:\u001b[32m351\u001b[39m)\n  org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(\u001b[32mDataSource.scala\u001b[39m:\u001b[32m648\u001b[39m)\n  scala.util.Try$.apply(\u001b[32mTry.scala\u001b[39m:\u001b[32m213\u001b[39m)\n  org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(\u001b[32mDataSource.scala\u001b[39m:\u001b[32m648\u001b[39m)\n  scala.util.Failure.orElse(\u001b[32mTry.scala\u001b[39m:\u001b[32m224\u001b[39m)\n  org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(\u001b[32mDataSource.scala\u001b[39m:\u001b[32m648\u001b[39m)\n  org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(\u001b[32mDataSource.scala\u001b[39m:\u001b[32m728\u001b[39m)\n  org.apache.spark.sql.DataFrameWriter.lookupV2Provider(\u001b[32mDataFrameWriter.scala\u001b[39m:\u001b[32m948\u001b[39m)\n  org.apache.spark.sql.DataFrameWriter.save(\u001b[32mDataFrameWriter.scala\u001b[39m:\u001b[32m285\u001b[39m)\n  org.apache.spark.sql.DataFrameWriter.save(\u001b[32mDataFrameWriter.scala\u001b[39m:\u001b[32m269\u001b[39m)\n  ammonite.$sess.cmd8$Helper.<init>(\u001b[32mcmd8.sc\u001b[39m:\u001b[32m13\u001b[39m)\n  ammonite.$sess.cmd8$.<init>(\u001b[32mcmd8.sc\u001b[39m:\u001b[32m7\u001b[39m)\n  ammonite.$sess.cmd8$.<clinit>(\u001b[32mcmd8.sc\u001b[39m:\u001b[32m-1\u001b[39m)"
     ]
    }
   ],
   "source": [
    "// Configure source data path\n",
    "val sourcePath = \"../databricks-datasets/learning-spark-v2/loans/loan-risks.snappy.parquet\"\n",
    "\n",
    "// Configure Delta Lake path\n",
    "val deltaPath = \"/tmp/loans_delta\"\n",
    "\n",
    "// Create the Delta table with the same loans data\n",
    "spark\n",
    "  .read\n",
    "  .format(\"parquet\")\n",
    "  .load(sourcePath)\n",
    "  .write\n",
    "  .format(\"delta\")\n",
    "  .save(deltaPath)\n",
    "\n",
    "// Create a view on the data called loans_delta\n",
    "spark\n",
    " .read\n",
    " .format(\"delta\")\n",
    " .load(deltaPath)\n",
    " .createOrReplaceTempView(\"loans_delta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.avg\n",
    "import org.apache.spark.sql.SparkSession\n",
    "\n",
    "\n",
    "import org.apache.spark.sql.SQLContext\n",
    "\n",
    "// Create a DataFrame using SparkSession\n",
    "val spark = SparkSession\n",
    "  .builder\n",
    "  .master(\"local[*]\")\n",
    "  .appName(\"AuthorsAges\")\n",
    "  .getOrCreate()\n",
    "\n",
    "import spark.implicits._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// In Scala\n",
    "// Configure source data path\n",
    "val sourcePath = \"../databricks-datasets/learning-spark-v2/loans/\"+\n",
    "                 \"loan-risks.snappy.parquet\"\n",
    "\n",
    "\n",
    "// Configure Delta Lake path\n",
    "val deltaPath = \"/tmp/loans_delta\"\n",
    "\n",
    "// Create the Delta table with the same loans data\n",
    "spark\n",
    "  .read\n",
    "  .format(\"parquet\")\n",
    "  .load(sourcePath)\n",
    "  .write\n",
    "  .format(\"delta\")\n",
    "  .save(deltaPath)\n",
    "\n",
    "// Create a view on the data called loans_delta\n",
    "spark\n",
    " .read\n",
    " .format(\"delta\")\n",
    " .load(deltaPath)\n",
    " .createOrReplaceTempView(\"loans_delta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
