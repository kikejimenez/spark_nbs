{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Learning Spark Second Edition](https://github.com/databricks/LearningSparkV2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " _all-spark-notebook_ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Chapter Four](https://learning.oreilly.com/library/view/learning-spark-2nd/9781492050032/ch04.html)\n",
    "> spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import path,popen\n",
    "#SPARK_HOME = popen('echo $SPARK_HOME').read().strip()\n",
    "PARENT_DIR = popen('dirname $PWD').read().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Python\n",
    "from pyspark.sql import SparkSession\n",
    "# Create a SparkSession\n",
    "spark = (SparkSession.builder.appName(\"SparkSQLExampleApp\").getOrCreate())\n",
    "\n",
    "# Path to data set\n",
    "csv_file = path.join(\n",
    "    PARENT_DIR,\n",
    "    \"databricks-datasets/learning-spark-v2/flights/departuredelays.csv\"\n",
    ")\n",
    "\n",
    "# Read and create a temporary view\n",
    "# Infer schema (note that for larger files you\n",
    "# may want to specify the schema)\n",
    "df = (\n",
    "    spark.read.format(\"csv\").option(\"inferSchema\",\n",
    "                                    \"true\").option(\"header\",\n",
    "                                                   \"true\").load(csv_file)\n",
    ")\n",
    "df.createOrReplaceTempView(\"us_delay_flights_tbl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+-----------+\n",
      "|distance|origin|destination|\n",
      "+--------+------+-----------+\n",
      "|    4330|   JFK|        HNL|\n",
      "|    4330|   JFK|        HNL|\n",
      "|    4330|   JFK|        HNL|\n",
      "|    4330|   JFK|        HNL|\n",
      "|    4330|   JFK|        HNL|\n",
      "|    4330|   JFK|        HNL|\n",
      "|    4330|   JFK|        HNL|\n",
      "|    4330|   JFK|        HNL|\n",
      "|    4330|   JFK|        HNL|\n",
      "|    4330|   JFK|        HNL|\n",
      "+--------+------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"\"\"SELECT distance, origin, destination \n",
    "FROM us_delay_flights_tbl WHERE distance > 1000 \n",
    "ORDER BY distance DESC\"\"\"\n",
    ").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+------+-----------+\n",
      "|   date|delay|origin|destination|\n",
      "+-------+-----+------+-----------+\n",
      "|2190925| 1638|   SFO|        ORD|\n",
      "|1031755|  396|   SFO|        ORD|\n",
      "|1022330|  326|   SFO|        ORD|\n",
      "|1051205|  320|   SFO|        ORD|\n",
      "|1190925|  297|   SFO|        ORD|\n",
      "|2171115|  296|   SFO|        ORD|\n",
      "|1071040|  279|   SFO|        ORD|\n",
      "|1051550|  274|   SFO|        ORD|\n",
      "|3120730|  266|   SFO|        ORD|\n",
      "|1261104|  258|   SFO|        ORD|\n",
      "+-------+-----+------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"\"\"SELECT date, delay, origin, destination \n",
    "FROM us_delay_flights_tbl \n",
    "WHERE delay > 120 AND ORIGIN = 'SFO' AND DESTINATION = 'ORD' \n",
    "ORDER by delay DESC\"\"\"\n",
    ").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+-----------+-------------+\n",
      "|delay|origin|destination|Flight_Delays|\n",
      "+-----+------+-----------+-------------+\n",
      "|  333|   ABE|        ATL|  Long Delays|\n",
      "|  305|   ABE|        ATL|  Long Delays|\n",
      "|  275|   ABE|        ATL|  Long Delays|\n",
      "|  257|   ABE|        ATL|  Long Delays|\n",
      "|  247|   ABE|        ATL|  Long Delays|\n",
      "|  247|   ABE|        DTW|  Long Delays|\n",
      "|  219|   ABE|        ORD|  Long Delays|\n",
      "|  211|   ABE|        ATL|  Long Delays|\n",
      "|  197|   ABE|        DTW|  Long Delays|\n",
      "|  192|   ABE|        ORD|  Long Delays|\n",
      "+-----+------+-----------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"\"\"SELECT delay, origin, destination,\n",
    "              CASE\n",
    "                  WHEN delay > 360 THEN 'Very Long Delays'\n",
    "                  WHEN delay > 120 AND delay < 360 THEN 'Long Delays'\n",
    "                  WHEN delay > 60 AND delay < 120 THEN 'Short Delays'\n",
    "                  WHEN delay > 0 and delay < 60  THEN  'Tolerable Delays'\n",
    "                  WHEN delay = 0 THEN 'No Delays'\n",
    "                  ELSE 'Early'\n",
    "               END AS Flight_Delays\n",
    "               FROM us_delay_flights_tbl\n",
    "               ORDER BY origin, delay DESC\"\"\"\n",
    ").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Chapter Four](https://learning.oreilly.com/library/view/learning-spark-2nd/9781492050032/ch04.html)\n",
    "> SQL like with DataFrame API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Python\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = (SparkSession.builder.appName(\"SparkSQLExampleApp\").getOrCreate())\n",
    "\n",
    "# Path to data set\n",
    "csv_file = path.join(\n",
    "    PARENT_DIR,\n",
    "    \"databricks-datasets/learning-spark-v2/flights/departuredelays.csv\"\n",
    ")\n",
    "\n",
    "# Read and create a temporary view. Infer schema (note that for larger files you\n",
    "# may want to specify the schema)\n",
    "df = (\n",
    "    spark.read.format(\"csv\").option(\"inferSchema\",\n",
    "                                    \"true\").option(\"header\",\n",
    "                                                   \"true\").load(csv_file)\n",
    ")\n",
    "df.createOrReplaceTempView(\"us_delay_flights_tbl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+-----------+\n",
      "|distance|origin|destination|\n",
      "+--------+------+-----------+\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "+--------+------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# In Python\n",
    "from pyspark.sql.functions import col, desc\n",
    "(\n",
    "    df.select(\"distance\", \"origin\",\n",
    "              \"destination\").where(col(\"distance\") > 1000).orderBy(\n",
    "                  desc(\"distance\")\n",
    "              )\n",
    ").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Chapter Four](https://learning.oreilly.com/library/view/learning-spark-2nd/9781492050032/ch04.html)\n",
    "> Managed and unmanaged tables (using DataFrame API)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#managed\n",
    "from os import path, popen\n",
    "from shutil import rmtree\n",
    "PARENT_DIR = popen('dirname $PWD').read().strip()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = (SparkSession.builder.appName(\"SparkSQLExampleApp\").getOrCreate())\n",
    "\n",
    "#remove default directory\n",
    "rmtree(\n",
    "    spark.catalog.listDatabases()[0].locationUri.split(':')[-1],\n",
    "    ignore_errors=True\n",
    ")\n",
    "spark.sql('''CREATE DATABASE learn_spark_db''')\n",
    "spark.sql(\"USE learn_spark_db\")\n",
    "\n",
    "# Path to our US flight delays CSV file\n",
    "csv_file = path.join(\n",
    "    PARENT_DIR,\n",
    "    \"databricks-datasets/learning-spark-v2/flights/departuredelays.csv\"\n",
    ")\n",
    "\n",
    "# Schema as defined in the preceding example\n",
    "schema = \"date STRING, delay INT, distance INT, origin STRING, destination STRING\"\n",
    "flights_df = spark.read.csv(csv_file, schema=schema)\n",
    "'''\n",
    "default_dbs = spark.catalog.listDatabases()\n",
    "if default_dbs:\n",
    "    \n",
    "    default_dir = default_dbs[0].locationUri.split(':')[-1]\n",
    "    if path.isdir(default_dir): \n",
    "        rmtree(default_dir)\n",
    "'''\n",
    "\n",
    "flights_df.write.saveAsTable(\"managed_us_delay_flights_tbl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unmanaged\n",
    "(\n",
    "    flights_df.write.option(\n",
    "        \"path\",\n",
    "        path.join(spark.catalog.listDatabases()[0].locationUri.split(':')[-1])\n",
    "    )\n",
    "    #,\n",
    "    #   \"us_flights_delay\"))\n",
    "    .saveAsTable(\"us_delay_flights_tbl\", mode='overwrite')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Chapter Four](https://learning.oreilly.com/library/view/learning-spark-2nd/9781492050032/ch04.html)\n",
    "> Managed and unmanaged tables (using DataFrame API)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import path, popen\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "PARENT_DIR = popen('dirname $PWD').read().strip()\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = (SparkSession.builder.appName(\"SparkSQLExampleApp\").getOrCreate())\n",
    "\n",
    "# Path to data set\n",
    "csv_file = path.join(\n",
    "    PARENT_DIR,\n",
    "    \"databricks-datasets/learning-spark-v2/flights/departuredelays.csv\"\n",
    ")\n",
    "\n",
    "# Read and create a temporary view. Infer schema (note that for larger files you\n",
    "# may want to specify the schema)\n",
    "df = (\n",
    "    spark.read.format(\"csv\").option(\"inferSchema\",\n",
    "                                    \"true\").option(\"header\",\n",
    "                                                   \"true\").load(csv_file)\n",
    ")\n",
    "df.createOrReplaceTempView(\"us_delay_flights_tbl\")\n",
    "\n",
    "df_sfo = spark.sql(\n",
    "    \"SELECT date, delay, origin, destination FROM \\\n",
    "  us_delay_flights_tbl WHERE origin = 'SFO'\"\n",
    ")\n",
    "df_jfk = spark.sql(\n",
    "    \"SELECT date, delay, origin, destination FROM \\\n",
    "  us_delay_flights_tbl WHERE origin = 'JFK'\"\n",
    ")\n",
    "\n",
    "# Create a temporary and global temporary view\n",
    "df_sfo.createOrReplaceGlobalTempView(\"us_origin_airport_SFO_global_tmp_view\")\n",
    "df_jfk.createOrReplaceTempView(\"us_origin_airport_JFK_tmp_view\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM us_origin_airport_JFK_tmp_view\")\n",
    "\n",
    "spark.catalog.dropGlobalTempView(\"us_origin_airport_SFO_global_tmp_view\")\n",
    "spark.catalog.dropTempView(\"us_origin_airport_JFK_tmp_view\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Chapter Four](https://learning.oreilly.com/library/view/learning-spark-2nd/9781492050032/ch04.html)\n",
    "> Catalog: Access metada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Database(name='default', description='default database', locationUri='file:/wd/notebooks/spark-warehouse')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mType:\u001b[0m        property\n",
       "\u001b[0;31mString form:\u001b[0m <property object at 0x7f50302a6310>\n",
       "\u001b[0;31mDocstring:\u001b[0m  \n",
       "Returns a :class:`DataFrameReader` that can be used to read data\n",
       "in as a :class:`DataFrame`.\n",
       "\n",
       ":return: :class:`DataFrameReader`\n",
       "\n",
       ".. versionadded:: 2.0\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from os import path, popen\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "PARENT_DIR = popen('dirname $PWD').read().strip()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = (SparkSession.builder.appName(\"SparkSQLExampleApp\").getOrCreate())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Chapter Four](https://learning.oreilly.com/library/view/learning-spark-2nd/9781492050032/ch04.html)\n",
    "> Read Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "PARENT_DIR = popen('dirname $PWD').read().strip()\n",
    "\n",
    "spark = (SparkSession.builder.appName(\"SparkSQLExampleApp\").getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parquet into DataFrame\n",
    "file = path.join(\n",
    "    popen('echo $PWD').read().strip(),\n",
    "    'spark-warehouse/learn_spark_db.db/managed_us_delay_flights_tbl/'\n",
    ")\n",
    "df = spark.read.format(\"parquet\").load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+--------+------+-----------+\n",
      "|    date|delay|distance|origin|destination|\n",
      "+--------+-----+--------+------+-----------+\n",
      "|02151800|  108|     290|   ORD|        MSP|\n",
      "|02151800|  142|     772|   ORD|        DEN|\n",
      "|02151303|   16|    1516|   ORD|        LAX|\n",
      "|02151157|    7|    1316|   ORD|        LAS|\n",
      "|02151818|   55|    1511|   ORD|        PDX|\n",
      "|02151033|   12|     873|   ORD|        MCO|\n",
      "|02150941|    0|    1499|   ORD|        SNA|\n",
      "|02151320|   17|    1604|   ORD|        SFO|\n",
      "|02151804|    2|    1497|   ORD|        SAN|\n",
      "|02152000|   17|     119|   ORD|        GRR|\n",
      "|02151502|   15|     260|   ORD|        DSM|\n",
      "|02151935|   73|     879|   ORD|        TPA|\n",
      "|02151315|   -1|     879|   ORD|        TPA|\n",
      "|02150957|    0|     939|   ORD|        MTJ|\n",
      "|02152001|  123|     804|   ORD|        IAH|\n",
      "|02151945|   42|     558|   ORD|        RIC|\n",
      "|02151502|   89|     625|   ORD|        EWR|\n",
      "|02150653|   -5|     974|   ORD|        RSW|\n",
      "|02150947|   -1|    1316|   ORD|        LAS|\n",
      "|02152030|   24|    1604|   ORD|        SFO|\n",
      "+--------+-----+--------+------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Parquet into SQL\n",
    "\n",
    "path = os.path.join(\n",
    "    popen('echo $PWD').read().strip(),\n",
    "    'spark-warehouse/learn_spark_db.db/managed_us_delay_flights_tbl'\n",
    ")\n",
    "\n",
    "spark.sql(\n",
    "    f'CREATE OR REPLACE TEMPORARY VIEW us_delay_flights_tbl  \\\n",
    "    USING parquet  OPTIONS (path \"{path}\")'\n",
    ")\n",
    "\n",
    "spark.sql(\"SELECT * FROM us_delay_flights_tbl\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WriteFile\n",
    "(df.write.format(\"parquet\")\n",
    "  .mode(\"overwrite\")\n",
    "  .option(\"compression\", \"snappy\")\n",
    "  .save(\"/tmp/data/parquet/df_parquet\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
